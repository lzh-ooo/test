{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1504d540-4be5-4e7e-9860-fa2c494dcbda",
   "metadata": {},
   "source": [
    "- Step 1.导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b85b0e7-a16c-407c-889b-a15e899bf0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/MateConv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29861abc-d6ea-4deb-a437-42601f73cfbc",
   "metadata": {},
   "source": [
    "- Step 2.读取 tokenizer_train.jsonl 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e1dc03-f9be-4ede-b8f3-5422afbf25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的。现在请你将这个文本中的所有的逗号都替换成空格。 好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？\n",
      "帮我回答一道历史题目。清朝时期的八旗共有多少旗人？ 清朝时期八旗旗人总数约为200万人左右，其中正黄旗、正蓝旗、正白旗、正红旗的人数较多，其他旗的人数较少。\n",
      "嗯，谢谢你介绍的做法很详细，但我不喜欢吃鸡蛋，有没有其他菜做法能介绍一下？ 当然，你可以试试酸辣土豆丝这道菜。\n",
      "材料：\n",
      "土豆2个、红椒1个、青椒1个、大葱1根、醋、生抽、盐、鸡精、料酒\n",
      "做法：\n",
      "1.土豆去皮，切成丝；红椒和青椒切成细丝；大葱切段备用。\n",
      "2.热锅凉油，油热后放入土豆丝，煸炒至变软。\n",
      "3.倒入红椒、青椒和大葱段，继续煸炒至熟。\n",
      "4.加入适量的盐、鸡精、料酒和生抽，翻炒均匀。\n",
      "5.最后，加入适量的醋，翻炒均匀即可。\n",
      "小贴士：\n",
      "1. 土豆切丝时，可以放入淡盐水中泡一下，这样可以去除多余的淀粉。\n",
      "2. 煮土豆丝时，不要煮得太久，以免烂糊。\n",
      "3. 加入醋的时候，根据自己的口味多少来进行调节，一般来说，盐与醋的比例为1:1。\n",
      "4. 如果喜欢辣味可以加入一些干辣椒丝。\n",
      "希望你会喜欢这道酸辣土豆丝！\n",
      "请描述一下如何正确规划个人理财。 正确规划个人理财需要以下几个步骤：\n",
      "1.了解自己的财务状况。这包括收入、支出、资产和负债等信息。了解自己的财务状况可以帮助人们更好地制定财务计划。\n",
      "2.设定财务目标。需要考虑短期目标和长期目标，例如以年为单位设定的支出计划、购房、购车等的长期目标。\n",
      "3.制定预算计划。在了解自己的财务状况并设定财务目标后，需要制定一个预算计划。这可以帮助人们控制支出、节省开支并达到财务目标。\n",
      "4.理性投资和储蓄。人们可以投资于股票、基金、房产或其他投资渠道以实现财务目标。但在投资前需了解相关知识并进行风险评估。同时还应储蓄一定金额，以应对突发事件或为达成某些目标做准备。\n",
      "5.审时度势，合理调整。财务计划需要不断地审时度势，根据实际情况做出调整，以达到最终的财务目标。需要注意财务状况的变化、投资的收益和风险等因素。\n",
      "通过以上五个步骤，人们可以做到合理规划个人理财，掌握自己的财务命运，更好地实现自己的财务目标。\n",
      "描述一下天堂和地狱的生态系统和环境。 天堂和地狱被认为是灵性信仰中关于死后世界的两种不同概念。然而，它们的生态系统和环境都是具有类似特征的极端不同的地方。以下是我对天堂和地狱的生态系统和环境的描述。\n",
      "天堂的生态系统和环境:\n",
      "天堂被描绘为一个美丽、平静、和谐的地方，类似于一片无垢的花园。天堂的生态系统和环境的特征包括:\n",
      "1. 充满和平和爱的氛围。这是一个没有恐惧、痛苦、疾病和死亡的地方。\n",
      "2. 色彩缤纷，充满生机。这是一个绿树成荫、花团锦簇的地方，充满生机和活力。\n",
      "3. 各种生物和动物和谐共存。天使、圣人和各种动物和谐相处，生态系统中没有互相侵害或抢夺资源。\n",
      "4. 充满清新气息的空气。没有污染、烟雾或其他有害物质，空气中充满清新芬芳的气息。\n",
      "5. 物质丰富的环境。天堂中生活着满足需求和愿望的人们，他们拥有一切所需的物质资源，而且没有匮乏、浪费或不公平。\n",
      "地狱的生态系统和环境:\n",
      "地狱被描绘为阴暗、恐怖、嘈杂和可怕的地方。地狱的生态系统和环境的特征包括:\n",
      "1. 充满痛苦和折磨的氛围。这是一个充满恐惧、悔恨和痛苦的地方，全是罪恶的味道。\n",
      "2. 火焰和烈火环绕。地狱中有燃烧的火焰和烈火，许多受罚者被投入火坑中痛苦折磨。\n",
      "3. 恶魔和妖魔横行。地狱中有恶魔、妖怪等可怕的生物，它们在无休止的受苦中享受着自己的又一场比赛。\n",
      "4. 污染和恶臭的气味。地狱中到处都是恶臭和污染，没有清新的气息。\n",
      "5. 没有物质资源。地狱中生活着被惩罚的人们不可能拥有任何物质财富，地狱环境充满了无尽的贫困、饥饿和疾病。\n",
      "综上所述，天堂和地狱是两个完全不同的地方，它们的生态系统和环境反映了它们的性质，体现了人类对不同阶段的死后生命的不同想象和信仰。\n"
     ]
    }
   ],
   "source": [
    "def read_texts_from_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            yield data['text']\n",
    "\n",
    "# 测试读取数据\n",
    "data_path = '/root/autodl-tmp/MateGenConv/dataset/tokenizer_train.jsonl'\n",
    "texts = read_texts_from_jsonl(data_path)\n",
    "\n",
    "# 打印前几行文本\n",
    "for i, text in enumerate(texts):\n",
    "    if i < 5:\n",
    "        print(text)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914305ae-436f-47ba-bd1f-6f3b7bbdce61",
   "metadata": {},
   "source": [
    "- 初始化分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3958bd2a-154c-4d63-ba75-82a36c92b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词器初始化成功，准备训练。\n"
     ]
    }
   ],
   "source": [
    "# 初始化tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# 定义特殊token\n",
    "special_tokens = [\"<unk>\", \"<s>\", \"</s>\"]\n",
    "\n",
    "# 设置训练器并添加特殊token\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=6400,\n",
    "    special_tokens=special_tokens,  # 确保这三个token被包含\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")\n",
    "\n",
    "print(\"分词器初始化成功，准备训练。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902f123-7c3e-4d73-be1a-0178ff896f33",
   "metadata": {},
   "source": [
    "- Step 4.训练分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd14d6e-a14f-47dd-aaa8-7b6e8e60daf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "分词器训练完成！\n"
     ]
    }
   ],
   "source": [
    "# 读取文本数据\n",
    "texts = read_texts_from_jsonl(data_path)\n",
    "\n",
    "# 训练tokenizer\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "\n",
    "print(\"分词器训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ec028-dd63-455d-9dfe-81c0a5882c98",
   "metadata": {},
   "source": [
    "- Step 5.保存分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b40976-3741-4b09-9147-08e031e31de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer 保存成功！\n"
     ]
    }
   ],
   "source": [
    "# 设置解码器\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# 保存tokenizer\n",
    "tokenizer_dir = \"./model/mateconv_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
    "tokenizer.model.save(\"./model/mateconv_tokenizer\")\n",
    "\n",
    "# 手动创建配置文件\n",
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": True,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<unk>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "            },\n",
    "        \"1\": {\n",
    "            \"content\": \"<s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "            },\n",
    "        \"2\": {\n",
    "            \"content\": \"</s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "            }\n",
    "    },\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 1000000000000000019884624838656,\n",
    "    \"pad_token\": None,\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"use_default_system_prompt\": False,\n",
    "    \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "}\n",
    "\n",
    "# 保存配置文件\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer 保存成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba1478-98f4-4433-b157-2b5f09b86b91",
   "metadata": {},
   "source": [
    "- Step 6.评估分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "555b238b-d349-4f03-a33c-50edd757eb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个优秀的聊天机器人，总是给我正确的回应！<s>user\n",
      "是椭圆形的</s>\n",
      "<s>assistant\n",
      "456</s>\n",
      "<s>user\n",
      "456</s>\n",
      "<s>assistant\n",
      "789</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/mateconv_tokenizer\")\n",
    "\n",
    "# 测试一段对话\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
    "    {\"role\": \"user\", \"content\": '是椭圆形的'},\n",
    "    {\"role\": \"assistant\", \"content\": '456'},\n",
    "    {\"role\": \"user\", \"content\": '456'},\n",
    "    {\"role\": \"assistant\", \"content\": '789'}\n",
    "]\n",
    "\n",
    "# 使用模板进行文本处理\n",
    "new_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ac14a-3c15-4985-b00a-f8aa5beac72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0f67f-47f2-4163-b0cc-8fdd2d1e8ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3264a-10d4-43f3-841f-289613ea41f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e45f83-b854-4a9e-adc4-456ad5afa75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1823314-d8d3-4131-8370-ced6cd7cd106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MateConv)",
   "language": "python",
   "name": "mateconv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
