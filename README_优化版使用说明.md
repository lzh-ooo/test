# MateConv é¢„è®­ç»ƒä¼˜åŒ–ç‰ˆ - ä½¿ç”¨è¯´æ˜

## ğŸ“š ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ•°æ®å¤„ç†](#æ•°æ®å¤„ç†)
3. [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
4. [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
5. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install torch transformers datasets jsonlines tqdm pyyaml numpy
```

### ç›®å½•ç»“æ„

```
Labelimg/
â”œâ”€â”€ model/                          # æ¨¡å‹ä»£ç ç›®å½•ï¼ˆéœ€è¦æ‚¨æä¾›ï¼‰
â”‚   â”œâ”€â”€ model.py                   # Transformer æ¨¡å‹
â”‚   â”œâ”€â”€ LMConfig.py               # æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ dataset.py                # æ•°æ®é›†ç±»
â”œâ”€â”€ dataset/                       # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ mobvoi_seq_monkey_general_open_corpus.jsonl  # åŸå§‹æ•°æ®
â”‚   â””â”€â”€ pretrain_data.bin         # å¤„ç†åçš„æ•°æ®
â”œâ”€â”€ out/                          # è¾“å‡ºç›®å½•
â”‚   â””â”€â”€ *.pth                     # æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ config.yaml                   # é…ç½®æ–‡ä»¶
â”œâ”€â”€ optimized_data_processing.py  # ä¼˜åŒ–çš„æ•°æ®å¤„ç†
â”œâ”€â”€ optimized_pretrain.py         # ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬
â”œâ”€â”€ optimized_inference.py        # ä¼˜åŒ–çš„æ¨ç†è„šæœ¬
â”œâ”€â”€ train.py                      # è®­ç»ƒå¯åŠ¨è„šæœ¬
â”œâ”€â”€ inference.py                  # æ¨ç†å¯åŠ¨è„šæœ¬
â””â”€â”€ utils.py                      # å·¥å…·å‡½æ•°
```

---

## ğŸ“Š æ•°æ®å¤„ç†

### æ–¹æ³• 1: ä½¿ç”¨ Python è„šæœ¬

```python
from optimized_data_processing import OptimizedDataProcessor

# åˆ›å»ºå¤„ç†å™¨
processor = OptimizedDataProcessor(
    tokenizer_path='/path/to/tokenizer',
    max_length=512,
    chunk_size=50000,
    save_interval=1000000
)

# éªŒè¯å’Œæ¸…æ´—æ•°æ®
processor.validate_and_clean_jsonl(
    input_path='./dataset/raw_data.jsonl',
    output_path='./dataset/cleaned_data.jsonl'
)

# å¤„ç†æ•°æ®é›†
processor.process_dataset(
    input_path='./dataset/cleaned_data.jsonl',
    output_path='./dataset/pretrain_data.bin',
    resume=True  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
)

# ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
processor.save_statistics('./dataset/data_stats.json')
```

### æ–¹æ³• 2: ç›´æ¥è¿è¡Œä¸»å‡½æ•°

```bash
python optimized_data_processing.py
```

### æ•°æ®æ ¼å¼è¦æ±‚

è¾“å…¥æ•°æ®åº”ä¸º JSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼š

```json
{"text": "è¿™æ˜¯ç¬¬ä¸€æ¡æ–‡æœ¬æ•°æ®"}
{"text": "è¿™æ˜¯ç¬¬äºŒæ¡æ–‡æœ¬æ•°æ®"}
```

---

## ğŸ¯ æ¨¡å‹è®­ç»ƒ

### æ–¹æ³• 1: ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰

**1. ç¼–è¾‘é…ç½®æ–‡ä»¶ `config.yaml`**

```yaml
data:
  tokenizer_path: "/path/to/tokenizer"
  train_data_path: "./dataset/pretrain_data.bin"
  val_data_path: null

training:
  epochs: 15
  batch_size: 32
  learning_rate: 3.0e-4
  optimizer_type: "adamw"
  lr_scheduler: "cosine"
  use_ema: true
  early_stopping: true
```

**2. å¯åŠ¨è®­ç»ƒ**

```bash
python train.py --config config.yaml
```

### æ–¹æ³• 2: ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°

```bash
python train.py \
    --epochs 15 \
    --batch_size 32 \
    --learning_rate 3e-4 \
    --optimizer_type adamw \
    --lr_scheduler cosine \
    --use_ema \
    --early_stopping \
    --out_dir out
```

### æ–¹æ³• 3: åˆ†å¸ƒå¼è®­ç»ƒ

**å•æœºå¤šå¡è®­ç»ƒï¼š**

```bash
# ä½¿ç”¨ torchrun (PyTorch >= 1.10)
torchrun --nproc_per_node=2 train.py \
    --config config.yaml \
    --ddp

# æˆ–ä½¿ç”¨ python -m torch.distributed.launch
python -m torch.distributed.launch \
    --nproc_per_node=2 \
    train.py --config config.yaml --ddp
```

### æ¢å¤è®­ç»ƒ

```bash
python train.py \
    --config config.yaml \
    --resume \
    --resume_from out/checkpoint_step_5000.pth
```

### è®­ç»ƒå‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--epochs` | è®­ç»ƒè½®æ•° | 15 |
| `--batch_size` | æ‰¹æ¬¡å¤§å° | 32 |
| `--learning_rate` | å­¦ä¹ ç‡ | 3e-4 |
| `--optimizer_type` | ä¼˜åŒ–å™¨ (adamw/adam) | adamw |
| `--lr_scheduler` | å­¦ä¹ ç‡è°ƒåº¦å™¨ (cosine/linear) | cosine |
| `--use_ema` | ä½¿ç”¨ EMA | False |
| `--grad_clip` | æ¢¯åº¦è£å‰ª | 1.0 |
| `--accumulation_steps` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | 1 |
| `--dtype` | æ•°æ®ç±»å‹ (float32/float16/bfloat16) | bfloat16 |
| `--early_stopping` | å¯ç”¨æ—©åœ | False |
| `--patience` | æ—©åœè€å¿ƒå€¼ | 5 |

---

## ğŸ’¬ æ¨¡å‹æ¨ç†

### æ–¹æ³• 1: å•æ¬¡ç”Ÿæˆ

```bash
python inference.py \
    --model_path out/pretrain_512.pth \
    --prompt "ä¸­å›½" \
    --max_new_tokens 100 \
    --temperature 0.8 \
    --top_k 50 \
    --top_p 0.9
```

### æ–¹æ³• 2: æµå¼è¾“å‡º

```bash
python inference.py \
    --model_path out/pretrain_512.pth \
    --prompt "é•¿æ±Ÿã€" \
    --stream
```

### æ–¹æ³• 3: äº¤äº’å¼å¯¹è¯

```bash
python inference.py \
    --model_path out/pretrain_512.pth \
    --interactive
```

äº¤äº’å¼å¯¹è¯ç¤ºä¾‹ï¼š

```
ğŸ¤– äº¤äº’å¼å¯¹è¯æ¨¡å¼
================================================================================
è¾“å…¥ 'exit'ã€'quit' æˆ– 'q' é€€å‡º
è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²
================================================================================

ğŸ‘¤ ç”¨æˆ·: ä½ å¥½
ğŸ¤– åŠ©æ‰‹: ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ

ğŸ‘¤ ç”¨æˆ·: ä»‹ç»ä¸€ä¸‹ä¸­å›½
ğŸ¤– åŠ©æ‰‹: ä¸­å›½æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å›½å®¶...

ğŸ‘¤ ç”¨æˆ·: exit
ğŸ‘‹ å†è§ï¼
```

### æ–¹æ³• 4: æ‰¹é‡ç”Ÿæˆ

**1. åˆ›å»ºæç¤ºæ–‡ä»¶ `prompts.txt`ï¼š**

```
ä¸­å›½
é•¿æ±Ÿã€é»„æ²³
ä½ å¥½ï¼Œå¥½ä¹…ä¸è§
```

**2. è¿è¡Œæ‰¹é‡ç”Ÿæˆï¼š**

```bash
python inference.py \
    --model_path out/pretrain_512.pth \
    --batch_file prompts.txt
```

### æ–¹æ³• 5: ä½¿ç”¨ Python API

```python
from transformers import AutoTokenizer
from model.model import Transformer
from model.LMConfig import LMConfig
from optimized_inference import TextGenerator, GenerationConfig

# åŠ è½½æ¨¡å‹
lm_config = LMConfig()
model = Transformer(lm_config)
model.load_state_dict(torch.load('out/pretrain_512.pth'))

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained('/path/to/tokenizer')

# åˆ›å»ºç”Ÿæˆå™¨
generator = TextGenerator(model, tokenizer)

# ç”Ÿæˆé…ç½®
gen_config = GenerationConfig(
    do_sample=True,
    temperature=0.8,
    top_k=50,
    top_p=0.9,
    max_new_tokens=100,
    repetition_penalty=1.2
)

# å•æ¬¡ç”Ÿæˆ
result = generator.generate("ä¸­å›½", gen_config)
print(result)

# æµå¼ç”Ÿæˆ
def callback(token):
    print(token, end="", flush=True)

result = generator.generate(
    "é•¿æ±Ÿã€",
    gen_config,
    stream=True,
    callback=callback
)

# æ‰¹é‡ç”Ÿæˆ
prompts = ["ä¸­å›½", "é•¿æ±Ÿã€", "ä½ å¥½"]
results = generator.batch_generate(prompts, gen_config)

# å¤šè½®å¯¹è¯
history = [
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"},
    {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ä¸­å›½"}
]
response = generator.chat(history, gen_config)
```

### é‡‡æ ·å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `--temperature` | æ¸©åº¦å‚æ•°ï¼Œè¶Šé«˜è¶Šéšæœº | 0.7-1.0 |
| `--top_k` | Top-K é‡‡æ ·ï¼Œä¿ç•™æ¦‚ç‡æœ€é«˜çš„ k ä¸ªè¯ | 40-50 |
| `--top_p` | Top-P é‡‡æ ·ï¼Œä¿ç•™ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„è¯ | 0.85-0.95 |
| `--repetition_penalty` | é‡å¤æƒ©ç½šï¼Œè¶Šé«˜è¶Šä¸é‡å¤ | 1.0-1.3 |
| `--no_repeat_ngram_size` | ç¦æ­¢é‡å¤çš„ n-gram å¤§å° | 0-3 |
| `--greedy` | ä½¿ç”¨è´ªå¿ƒè§£ç ï¼ˆæœ€ç¡®å®šï¼‰ | - |

**é‡‡æ ·ç­–ç•¥å»ºè®®ï¼š**

- **åˆ›æ„å†™ä½œ**: `temperature=1.0, top_k=50, top_p=0.95`
- **æ—¥å¸¸å¯¹è¯**: `temperature=0.8, top_k=40, top_p=0.9`
- **äº‹å®æ€§å›ç­”**: `temperature=0.5, top_k=20, top_p=0.85` æˆ–ä½¿ç”¨ `--greedy`

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. ä½¿ç”¨ Wandb è·Ÿè¸ªè®­ç»ƒ

```bash
pip install wandb
wandb login

python train.py \
    --config config.yaml \
    --use_wandb \
    --wandb_project "MateConv-Pretrain" \
    --wandb_run_name "exp1"
```

### 2. æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§ Batchï¼‰

```bash
# ç­‰æ•ˆäº batch_size=128 (32 * 4)
python train.py \
    --batch_size 32 \
    --accumulation_steps 4
```

### 3. æ··åˆç²¾åº¦è®­ç»ƒ

```bash
# ä½¿ç”¨ BF16ï¼ˆæ¨èåœ¨ Ampere æ¶æ„ GPU ä¸Šä½¿ç”¨ï¼‰
python train.py --dtype bfloat16

# ä½¿ç”¨ FP16ï¼ˆé€šç”¨ï¼‰
python train.py --dtype float16

# ä½¿ç”¨ FP32ï¼ˆæœ€é«˜ç²¾åº¦ä½†æœ€æ…¢ï¼‰
python train.py --dtype float32
```

### 4. æ¨¡å‹ EMAï¼ˆæå‡ç”Ÿæˆè´¨é‡ï¼‰

```bash
python train.py --use_ema
```

### 5. éªŒè¯é›†å’Œæ—©åœ

```yaml
# config.yaml
data:
  val_data_path: "./dataset/val_data.bin"

training:
  early_stopping: true
  patience: 5
  eval_interval: 500
```

### 6. è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦

```bash
# ä½™å¼¦é€€ç«ï¼ˆæ¨èï¼‰
python train.py --lr_scheduler cosine

# çº¿æ€§è¡°å‡
python train.py --lr_scheduler linear

# å¤šé¡¹å¼è¡°å‡
python train.py --lr_scheduler polynomial
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒåŠ é€Ÿ

1. **ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ**: `--dtype bfloat16` æˆ– `--dtype float16`
2. **å¢å¤§ batch size**: åœ¨æ˜¾å­˜å…è®¸çš„æƒ…å†µä¸‹å°½å¯èƒ½å¤§
3. **ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯**: æ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨ `--accumulation_steps`
4. **å¤šå¡è®­ç»ƒ**: ä½¿ç”¨ `--ddp` å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
5. **å‡å°‘æ—¥å¿—é¢‘ç‡**: `--log_interval 100`

### æ˜¾å­˜ä¼˜åŒ–

1. **é™ä½ batch size**: `--batch_size 16`
2. **ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯**: `--accumulation_steps 4`
3. **ä½¿ç”¨æ··åˆç²¾åº¦**: `--dtype float16`
4. **å‡å°‘æœ€å¤§åºåˆ—é•¿åº¦**: ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ `max_seq_len`

### ç”ŸæˆåŠ é€Ÿ

1. **ä½¿ç”¨ KV Cache**: `use_cache=True`ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
2. **å‡å°‘ç”Ÿæˆé•¿åº¦**: `--max_new_tokens 50`
3. **ä½¿ç”¨è´ªå¿ƒè§£ç **: `--greedy`
4. **æ‰¹é‡ç”Ÿæˆ**: ä½¿ç”¨ `batch_generate()` è€Œä¸æ˜¯å¾ªç¯è°ƒç”¨

---

## â“ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A:** å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
```bash
python train.py \
    --batch_size 8 \
    --accumulation_steps 4 \
    --dtype float16
```

### Q2: å¦‚ä½•æ¢å¤ä¸­æ–­çš„è®­ç»ƒï¼Ÿ

**A:**
```bash
python train.py \
    --resume \
    --resume_from out/checkpoint_interrupted.pth
```

### Q3: ç”Ÿæˆçš„æ–‡æœ¬é‡å¤æ€ä¹ˆåŠï¼Ÿ

**A:** å¢åŠ é‡å¤æƒ©ç½šå’Œä½¿ç”¨ ngram é˜»æ­¢ï¼š
```bash
python inference.py \
    --repetition_penalty 1.3 \
    --no_repeat_ngram_size 3
```

### Q4: å¦‚ä½•ä½¿ç”Ÿæˆæ›´æœ‰åˆ›æ„/æ›´ä¿å®ˆï¼Ÿ

**A:** è°ƒæ•´æ¸©åº¦å‚æ•°ï¼š
- **æ›´æœ‰åˆ›æ„**: `--temperature 1.2`
- **æ›´ä¿å®ˆ**: `--temperature 0.5` æˆ– `--greedy`

### Q5: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**A:**
1. ä½¿ç”¨æ··åˆç²¾åº¦: `--dtype bfloat16`
2. å¢å¤§ batch size: `--batch_size 64`
3. ä½¿ç”¨å¤šå¡è®­ç»ƒ: `--ddp`
4. å‡å°‘éªŒè¯é¢‘ç‡: `--eval_interval 1000`

### Q6: å¦‚ä½•æŸ¥çœ‹è®­ç»ƒè¿›åº¦ï¼Ÿ

**A:** ä½¿ç”¨ Wandbï¼š
```bash
python train.py --use_wandb --wandb_project "my-project"
```
ç„¶åè®¿é—® https://wandb.ai æŸ¥çœ‹å¯è§†åŒ–

---

## ğŸ“ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: ä»é›¶å¼€å§‹è®­ç»ƒ

```bash
# 1. å¤„ç†æ•°æ®
python optimized_data_processing.py

# 2. è®­ç»ƒæ¨¡å‹
python train.py \
    --epochs 15 \
    --batch_size 32 \
    --learning_rate 3e-4 \
    --use_ema \
    --use_wandb

# 3. æµ‹è¯•ç”Ÿæˆ
python inference.py \
    --model_path out/best_model.pth \
    --prompt "ä¸­å›½" \
    --stream
```

### ç¤ºä¾‹ 2: åˆ†å¸ƒå¼è®­ç»ƒ + æ··åˆç²¾åº¦

```bash
torchrun --nproc_per_node=4 train.py \
    --config config.yaml \
    --ddp \
    --dtype bfloat16 \
    --batch_size 64 \
    --use_ema \
    --use_wandb
```

### ç¤ºä¾‹ 3: äº¤äº’å¼å¯¹è¯

```bash
python inference.py \
    --model_path out/best_model.pth \
    --interactive \
    --temperature 0.8 \
    --top_p 0.9 \
    --repetition_penalty 1.2 \
    --stream
```

---

## ğŸ‰ ä¼˜åŒ–æ€»ç»“

ç›¸æ¯”åŸå§‹ä»£ç ï¼Œæœ¬ä¼˜åŒ–ç‰ˆæœ¬æä¾›äº†ï¼š

### æ•°æ®å¤„ç†ä¼˜åŒ–
- âœ… åˆå¹¶æ¸…æ´—å’Œé¢„å¤„ç†æ­¥éª¤
- âœ… æ·»åŠ æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
- âœ… è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡å’Œå¯è§†åŒ–
- âœ… æ›´å¥½çš„é”™è¯¯å¤„ç†

### è®­ç»ƒä¼˜åŒ–
- âœ… å¤šç§ä¼˜åŒ–å™¨é€‰æ‹© (AdamW, Adam)
- âœ… çµæ´»çš„å­¦ä¹ ç‡è°ƒåº¦ (Cosine, Linear, Polynomial)
- âœ… æ¨¡å‹ EMA æå‡ç”Ÿæˆè´¨é‡
- âœ… éªŒè¯é›†å’Œæ—©åœæœºåˆ¶
- âœ… å®Œå–„çš„æ£€æŸ¥ç‚¹ç®¡ç†
- âœ… Wandb é›†æˆ
- âœ… åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

### æ¨ç†ä¼˜åŒ–
- âœ… å¤šç§é‡‡æ ·ç­–ç•¥ (Greedy, Top-K, Top-P, Temperature)
- âœ… KV Cache åŠ é€Ÿç”Ÿæˆ
- âœ… æ‰¹é‡ç”Ÿæˆæ”¯æŒ
- âœ… æµå¼è¾“å‡º
- âœ… äº¤äº’å¼å¯¹è¯
- âœ… é‡å¤æƒ©ç½šå’Œ ngram é˜»æ­¢

### ä»£ç è´¨é‡
- âœ… æ¸…æ™°çš„ä»£ç ç»“æ„
- âœ… è¯¦ç»†çš„æ³¨é‡Š
- âœ… é…ç½®æ–‡ä»¶æ”¯æŒ
- âœ… å®Œå–„çš„é”™è¯¯å¤„ç†
- âœ… æ˜“äºæ‰©å±•

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä»£ç æ³¨é‡Šæˆ–æäº¤ Issueã€‚

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€

