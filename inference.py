"""
ç®€åŒ–çš„æ¨ç†å¯åŠ¨è„šæœ¬
ç”¨æ³•:
    # å•æ¬¡ç”Ÿæˆ
    python inference.py --prompt "ä¸­å›½" --model_path out/pretrain_512.pth
    
    # äº¤äº’å¼å¯¹è¯
    python inference.py --model_path out/pretrain_512.pth --interactive
    
    # æ‰¹é‡ç”Ÿæˆ
    python inference.py --model_path out/pretrain_512.pth --batch_file prompts.txt
"""

import os
import sys
import argparse
import torch
from pathlib import Path

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from model.model import Transformer
    from model.LMConfig import LMConfig
except ImportError:
    print("âš ï¸  è­¦å‘Š: æ— æ³•å¯¼å…¥æ¨¡å‹æ¨¡å—")
    print("   è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è„šæœ¬ï¼Œéœ€è¦æ‚¨çš„å®é™…æ¨¡å‹ä»£ç ")
    
    class LMConfig:
        def __init__(self):
            self.dim = 512
            self.n_layers = 8
            self.n_heads = 16
            self.vocab_size = 6400
            self.max_seq_len = 512
            self.dropout = 0.1
            self.use_moe = False
    
    class Transformer:
        pass

from transformers import AutoTokenizer
from utils import load_config, get_device
from optimized_inference import TextGenerator, GenerationConfig


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="MateConv æ–‡æœ¬ç”Ÿæˆ")
    
    # æ¨¡å‹ç›¸å…³
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="æ¨¡å‹æƒé‡è·¯å¾„"
    )
    parser.add_argument(
        "--tokenizer_path",
        type=str,
        default="/root/autodl-tmp/MateGenConv/model/mateconv_tokenizer",
        help="åˆ†è¯å™¨è·¯å¾„"
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--prompt", type=str, default=None, help="è¾“å…¥æç¤º")
    parser.add_argument("--max_new_tokens", type=int, default=100, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--temperature", type=float, default=0.8, help="æ¸©åº¦å‚æ•°")
    parser.add_argument("--top_k", type=int, default=50, help="Top-K é‡‡æ ·")
    parser.add_argument("--top_p", type=float, default=0.9, help="Top-P é‡‡æ ·")
    parser.add_argument("--repetition_penalty", type=float, default=1.2, help="é‡å¤æƒ©ç½š")
    parser.add_argument("--no_repeat_ngram_size", type=int, default=3, help="ç¦æ­¢é‡å¤çš„ n-gram å¤§å°")
    parser.add_argument("--do_sample", action="store_true", default=True, help="æ˜¯å¦é‡‡æ ·")
    parser.add_argument("--greedy", action="store_true", help="ä½¿ç”¨è´ªå¿ƒè§£ç ï¼ˆè¦†ç›– do_sampleï¼‰")
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument("--interactive", action="store_true", help="äº¤äº’å¼å¯¹è¯æ¨¡å¼")
    parser.add_argument("--batch_file", type=str, default=None, help="æ‰¹é‡ç”Ÿæˆæ–‡ä»¶")
    parser.add_argument("--stream", action="store_true", help="æµå¼è¾“å‡º")
    
    # è®¾å¤‡
    parser.add_argument("--device", type=str, default="auto", help="è®¡ç®—è®¾å¤‡")
    
    return parser.parse_args()


def load_model(model_path: str, tokenizer_path: str, device: torch.device):
    """
    åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        tokenizer_path: åˆ†è¯å™¨è·¯å¾„
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        (model, tokenizer) å…ƒç»„
    """
    print("ğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)
    print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ (è¯è¡¨å¤§å°: {len(tokenizer)})")
    
    print("\nğŸ—ï¸  åˆå§‹åŒ–æ¨¡å‹...")
    lm_config = LMConfig()
    
    try:
        model = Transformer(lm_config)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        state_dict = torch.load(model_path, map_location=device)
        model.load_state_dict(state_dict)
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        
        model.to(device)
        model.eval()
        
        # ç»Ÿè®¡å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params / 1e6:.2f}M")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


def single_generation(generator: TextGenerator, args):
    """å•æ¬¡ç”Ÿæˆ"""
    if not args.prompt:
        print("âŒ è¯·æä¾›è¾“å…¥æç¤º (--prompt)")
        return
    
    # åˆ›å»ºç”Ÿæˆé…ç½®
    gen_config = GenerationConfig(
        do_sample=(not args.greedy),
        temperature=args.temperature,
        top_k=args.top_k,
        top_p=args.top_p,
        repetition_penalty=args.repetition_penalty,
        no_repeat_ngram_size=args.no_repeat_ngram_size,
        max_new_tokens=args.max_new_tokens
    )
    
    print("\n" + "=" * 80)
    print("ğŸ’¬ æ–‡æœ¬ç”Ÿæˆ")
    print("=" * 80)
    print(f"è¾“å…¥: {args.prompt}")
    print("-" * 80)
    
    if args.stream:
        print("è¾“å‡º: ", end="", flush=True)
        
        def callback(token: str):
            print(token, end="", flush=True)
        
        result = generator.generate(
            args.prompt,
            gen_config,
            stream=True,
            callback=callback
        )
        print("\n" + "=" * 80)
    else:
        result = generator.generate(args.prompt, gen_config)
        print(f"è¾“å‡º: {result}")
        print("=" * 80)


def interactive_mode(generator: TextGenerator, args):
    """äº¤äº’å¼å¯¹è¯æ¨¡å¼"""
    print("\n" + "=" * 80)
    print("ğŸ¤– äº¤äº’å¼å¯¹è¯æ¨¡å¼")
    print("=" * 80)
    print("è¾“å…¥ 'exit'ã€'quit' æˆ– 'q' é€€å‡º")
    print("è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("=" * 80 + "\n")
    
    # åˆ›å»ºç”Ÿæˆé…ç½®
    gen_config = GenerationConfig(
        do_sample=(not args.greedy),
        temperature=args.temperature,
        top_k=args.top_k,
        top_p=args.top_p,
        repetition_penalty=args.repetition_penalty,
        no_repeat_ngram_size=args.no_repeat_ngram_size,
        max_new_tokens=args.max_new_tokens
    )
    
    history = []
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ğŸ‘¤ ç”¨æˆ·: ").strip()
            
            if not user_input:
                continue
            
            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if user_input.lower() in ['exit', 'quit', 'q']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            # æ£€æŸ¥æ¸…ç©ºå‘½ä»¤
            if user_input.lower() == 'clear':
                history = []
                print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º\n")
                continue
            
            # æ·»åŠ åˆ°å†å²
            history.append({"role": "user", "content": user_input})
            
            # ç”Ÿæˆå›å¤
            if args.stream:
                print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
                
                def callback(token: str):
                    print(token, end="", flush=True)
                
                response = generator.chat(history, gen_config)
                print()
            else:
                response = generator.chat(history, gen_config)
                print(f"ğŸ¤– åŠ©æ‰‹: {response}")
            
            # æ·»åŠ å›å¤åˆ°å†å²
            history.append({"role": "assistant", "content": response})
            print()
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå‡ºé”™: {e}")
            continue


def batch_generation(generator: TextGenerator, args):
    """æ‰¹é‡ç”Ÿæˆ"""
    if not args.batch_file or not os.path.exists(args.batch_file):
        print(f"âŒ æ‰¹é‡æ–‡ä»¶ä¸å­˜åœ¨: {args.batch_file}")
        return
    
    print(f"\nğŸ“„ è¯»å–æ‰¹é‡æ–‡ä»¶: {args.batch_file}")
    
    # è¯»å–æç¤º
    with open(args.batch_file, 'r', encoding='utf-8') as f:
        prompts = [line.strip() for line in f if line.strip()]
    
    print(f"âœ… å…± {len(prompts)} ä¸ªæç¤º")
    
    # åˆ›å»ºç”Ÿæˆé…ç½®
    gen_config = GenerationConfig(
        do_sample=(not args.greedy),
        temperature=args.temperature,
        top_k=args.top_k,
        top_p=args.top_p,
        repetition_penalty=args.repetition_penalty,
        no_repeat_ngram_size=args.no_repeat_ngram_size,
        max_new_tokens=args.max_new_tokens
    )
    
    # æ‰¹é‡ç”Ÿæˆ
    print("\nğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ...\n")
    print("=" * 80)
    
    results = generator.batch_generate(prompts, gen_config)
    
    for i, (prompt, result) in enumerate(zip(prompts, results), 1):
        print(f"\n[{i}/{len(prompts)}]")
        print(f"è¾“å…¥: {prompt}")
        print(f"è¾“å‡º: {result}")
        print("-" * 80)
    
    # ä¿å­˜ç»“æœ
    output_file = args.batch_file.replace('.txt', '_output.txt')
    with open(output_file, 'w', encoding='utf-8') as f:
        for prompt, result in zip(prompts, results):
            f.write(f"è¾“å…¥: {prompt}\n")
            f.write(f"è¾“å‡º: {result}\n")
            f.write("-" * 80 + "\n")
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("ğŸš€ MateConv æ–‡æœ¬ç”Ÿæˆè„šæœ¬")
    print("=" * 80 + "\n")
    
    # è§£æå‚æ•°
    args = parse_args()
    
    # è·å–è®¾å¤‡
    device = get_device(args.device)
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    try:
        model, tokenizer = load_model(args.model_path, args.tokenizer_path, device)
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ä¸”æ¨¡å‹æ–‡ä»¶å­˜åœ¨")
        return
    
    # åˆ›å»ºç”Ÿæˆå™¨
    print("\nâš™ï¸  åˆ›å»ºæ–‡æœ¬ç”Ÿæˆå™¨...")
    generator = TextGenerator(model, tokenizer, device=str(device))
    print("âœ… ç”Ÿæˆå™¨åˆ›å»ºæˆåŠŸ")
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œ
    try:
        if args.interactive:
            # äº¤äº’å¼æ¨¡å¼
            interactive_mode(generator, args)
            
        elif args.batch_file:
            # æ‰¹é‡ç”Ÿæˆæ¨¡å¼
            batch_generation(generator, args)
            
        else:
            # å•æ¬¡ç”Ÿæˆæ¨¡å¼
            single_generation(generator, args)
            
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()

