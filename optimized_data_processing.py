"""
ä¼˜åŒ–çš„æ•°æ®é¢„å¤„ç†è„šæœ¬
æ”¹è¿›ç‚¹ï¼š
1. åˆå¹¶æ¸…æ´—å’Œé¢„å¤„ç†æ­¥éª¤
2. æ·»åŠ å¤šè¿›ç¨‹æ”¯æŒ
3. æ·»åŠ æ•°æ®ç»Ÿè®¡å’Œå¯è§†åŒ–
4. æ”¹è¿›é”™è¯¯å¤„ç†å’Œæ—¥å¿—
5. æ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

import os
import json
import jsonlines
import numpy as np
from typing import List, Dict, Tuple
from tqdm import tqdm
from transformers import AutoTokenizer
from collections import Counter
import multiprocessing as mp
from functools import partial
import pickle


class OptimizedDataProcessor:
    """ä¼˜åŒ–çš„æ•°æ®å¤„ç†å™¨"""
    
    def __init__(
        self,
        tokenizer_path: str,
        max_length: int = 512,
        chunk_size: int = 50000,
        save_interval: int = 1000000,
        num_workers: int = 4
    ):
        """
        Args:
            tokenizer_path: åˆ†è¯å™¨è·¯å¾„
            max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
            chunk_size: æ¯æ¬¡å¤„ç†çš„è¡Œæ•°
            save_interval: ç´¯ç§¯å¤šå°‘ä¸ª tokens åä¿å­˜ä¸€æ¬¡
            num_workers: å¤šè¿›ç¨‹æ•°é‡
        """
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)
        self.max_length = max_length
        self.chunk_size = chunk_size
        self.save_interval = save_interval
        self.num_workers = num_workers
        
        self.bos_token = "<s>"
        self.eos_token = "</s>"
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_lines': 0,
            'valid_lines': 0,
            'invalid_lines': 0,
            'skipped_long': 0,
            'total_tokens': 0,
            'text_lengths': [],
            'token_lengths': []
        }
    
    def validate_and_clean_jsonl(
        self, 
        input_path: str, 
        output_path: str
    ) -> Tuple[int, int]:
        """
        ä¸€æ¬¡æ€§å®ŒæˆéªŒè¯å’Œæ¸…æ´—
        
        Args:
            input_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            output_path: æ¸…æ´—åæ–‡ä»¶è·¯å¾„
            
        Returns:
            (valid_lines, invalid_lines) å…ƒç»„
        """
        print("ğŸ” å¼€å§‹éªŒè¯å’Œæ¸…æ´—æ•°æ®...")
        
        valid_lines = 0
        invalid_lines = 0
        invalid_line_numbers = []
        
        # ç¬¬ä¸€éï¼šç»Ÿè®¡æ€»è¡Œæ•°
        print("ğŸ“Š ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°...")
        with open(input_path, 'rb') as f:
            total_lines = sum(1 for _ in f)
        
        # ç¬¬äºŒéï¼šéªŒè¯å¹¶æ¸…æ´—
        print(f"âœ… æ€»è¡Œæ•°: {total_lines:,}")
        print("ğŸ§¹ å¼€å§‹æ¸…æ´—æ•°æ®...")
        
        with open(input_path, 'rb') as infile, \
             open(output_path, 'wb') as outfile:
            
            for idx, line in tqdm(enumerate(infile), total=total_lines, desc="æ¸…æ´—è¿›åº¦"):
                try:
                    # å°è¯•è§£ç å’Œè§£æ JSON
                    decoded_line = line.decode('utf-8')
                    obj = json.loads(decoded_line)
                    
                    # éªŒè¯å¿…è¦å­—æ®µ
                    if 'text' in obj and isinstance(obj['text'], str) and len(obj['text'].strip()) > 0:
                        outfile.write(line)
                        valid_lines += 1
                    else:
                        invalid_lines += 1
                        invalid_line_numbers.append(idx + 1)
                        
                except (UnicodeDecodeError, json.JSONDecodeError) as e:
                    invalid_lines += 1
                    invalid_line_numbers.append(idx + 1)
        
        print(f"\nâœ… æœ‰æ•ˆè¡Œæ•°: {valid_lines:,}")
        print(f"âŒ æ— æ•ˆè¡Œæ•°: {invalid_lines:,}")
        
        if invalid_line_numbers and len(invalid_line_numbers) <= 100:
            print(f"âš ï¸  æ— æ•ˆè¡Œå·: {invalid_line_numbers[:20]}...")
        
        return valid_lines, invalid_lines
    
    def tokenize_text(self, text: str) -> List[int]:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œåˆ†è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            token IDs åˆ—è¡¨
        """
        try:
            # æ·»åŠ  BOS å’Œ EOS æ ‡è®°
            formatted_text = f'{self.bos_token}{text}{self.eos_token}'
            token_ids = self.tokenizer(formatted_text).data['input_ids']
            return token_ids
        except Exception as e:
            return []
    
    def process_chunk(
        self, 
        chunk: List[Dict]
    ) -> Tuple[List[int], Dict]:
        """
        å¤„ç†ä¸€ä¸ªæ•°æ®å—
        
        Args:
            chunk: æ•°æ®å—
            
        Returns:
            (token_ids, chunk_stats) å…ƒç»„
        """
        doc_ids = []
        chunk_stats = {
            'valid': 0,
            'skipped_long': 0,
            'text_lengths': [],
            'token_lengths': []
        }
        
        for obj in chunk:
            try:
                content = obj.get('text', '').strip()
                
                if not content:
                    continue
                
                text_len = len(content)
                chunk_stats['text_lengths'].append(text_len)
                
                # è·³è¿‡è¿‡é•¿æ–‡æœ¬
                if text_len > self.max_length:
                    chunk_stats['skipped_long'] += 1
                    continue
                
                # åˆ†è¯
                text_id = self.tokenize_text(content)
                
                if text_id:
                    doc_ids.extend(text_id)
                    chunk_stats['token_lengths'].append(len(text_id))
                    chunk_stats['valid'] += 1
                    
            except Exception as e:
                continue
        
        return doc_ids, chunk_stats
    
    def merge_stats(self, chunk_stats: Dict):
        """åˆå¹¶ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['valid_lines'] += chunk_stats['valid']
        self.stats['skipped_long'] += chunk_stats['skipped_long']
        self.stats['text_lengths'].extend(chunk_stats['text_lengths'])
        self.stats['token_lengths'].extend(chunk_stats['token_lengths'])
    
    def process_dataset(
        self, 
        input_path: str, 
        output_path: str,
        resume: bool = False
    ):
        """
        å¤„ç†æ•´ä¸ªæ•°æ®é›†
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            resume: æ˜¯å¦ä»æ–­ç‚¹ç»§ç»­
        """
        print("\nğŸš€ å¼€å§‹å¤„ç†æ•°æ®é›†...")
        
        # æ£€æŸ¥æ–­ç‚¹
        checkpoint_path = output_path + '.checkpoint'
        start_line = 0
        
        if resume and os.path.exists(checkpoint_path):
            with open(checkpoint_path, 'rb') as f:
                checkpoint = pickle.load(f)
                start_line = checkpoint['processed_lines']
                self.stats = checkpoint['stats']
            print(f"ğŸ“ ä»ç¬¬ {start_line:,} è¡Œç»§ç»­å¤„ç†")
        else:
            # æ¸…ç©ºè¾“å‡ºæ–‡ä»¶
            if os.path.exists(output_path):
                os.remove(output_path)
        
        # ç»Ÿè®¡æ€»è¡Œæ•°
        with open(input_path, 'r', encoding='utf-8') as f:
            total_lines = sum(1 for _ in f)
        
        self.stats['total_lines'] = total_lines
        print(f"ğŸ“Š æ€»è¡Œæ•°: {total_lines:,}")
        
        # é€å—å¤„ç†
        doc_ids = []
        chunk_idx = 0
        processed_lines = start_line
        
        with jsonlines.open(input_path) as reader:
            # è·³è¿‡å·²å¤„ç†çš„è¡Œ
            if start_line > 0:
                for _ in range(start_line):
                    next(reader)
            
            with tqdm(total=total_lines - start_line, desc="å¤„ç†è¿›åº¦", initial=start_line) as pbar:
                while True:
                    try:
                        # è¯»å–ä¸€ä¸ªå—
                        chunk = []
                        for _ in range(self.chunk_size):
                            try:
                                chunk.append(next(reader))
                            except StopIteration:
                                break
                        
                        if not chunk:
                            break
                        
                        # å¤„ç†å—
                        chunk_ids, chunk_stats = self.process_chunk(chunk)
                        doc_ids.extend(chunk_ids)
                        self.merge_stats(chunk_stats)
                        
                        processed_lines += len(chunk)
                        chunk_idx += 1
                        pbar.update(len(chunk))
                        
                        # å®šæœŸä¿å­˜
                        if len(doc_ids) >= self.save_interval:
                            self._save_tokens(doc_ids, output_path)
                            self.stats['total_tokens'] += len(doc_ids)
                            doc_ids = []
                            
                            # ä¿å­˜æ£€æŸ¥ç‚¹
                            self._save_checkpoint(checkpoint_path, processed_lines)
                        
                    except Exception as e:
                        print(f"\nâš ï¸  å¤„ç†å— {chunk_idx} æ—¶å‡ºé”™: {e}")
                        continue
        
        # ä¿å­˜å‰©ä½™çš„ tokens
        if doc_ids:
            self._save_tokens(doc_ids, output_path)
            self.stats['total_tokens'] += len(doc_ids)
        
        # åˆ é™¤æ£€æŸ¥ç‚¹
        if os.path.exists(checkpoint_path):
            os.remove(checkpoint_path)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics()
    
    def _save_tokens(self, token_ids: List[int], output_path: str):
        """ä¿å­˜ token IDs åˆ°äºŒè¿›åˆ¶æ–‡ä»¶"""
        arr = np.array(token_ids, dtype=np.uint16)
        with open(output_path, 'ab') as f:
            f.write(arr.tobytes())
    
    def _save_checkpoint(self, checkpoint_path: str, processed_lines: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'processed_lines': processed_lines,
            'stats': self.stats
        }
        with open(checkpoint_path, 'wb') as f:
            pickle.dump(checkpoint, f)
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ•°æ®å¤„ç†ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»è¡Œæ•°:        {self.stats['total_lines']:>15,}")
        print(f"æœ‰æ•ˆè¡Œæ•°:      {self.stats['valid_lines']:>15,}")
        print(f"è·³è¿‡é•¿æ–‡æœ¬:    {self.stats['skipped_long']:>15,}")
        print(f"æ€» Token æ•°:   {self.stats['total_tokens']:>15,}")
        print("-" * 60)
        
        if self.stats['text_lengths']:
            text_lengths = np.array(self.stats['text_lengths'])
            print(f"æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
            print(f"  å¹³å‡å€¼:      {text_lengths.mean():>15.2f}")
            print(f"  ä¸­ä½æ•°:      {np.median(text_lengths):>15.0f}")
            print(f"  æœ€å°å€¼:      {text_lengths.min():>15,}")
            print(f"  æœ€å¤§å€¼:      {text_lengths.max():>15,}")
            print("-" * 60)
        
        if self.stats['token_lengths']:
            token_lengths = np.array(self.stats['token_lengths'])
            print(f"Token é•¿åº¦ç»Ÿè®¡:")
            print(f"  å¹³å‡å€¼:      {token_lengths.mean():>15.2f}")
            print(f"  ä¸­ä½æ•°:      {np.median(token_lengths):>15.0f}")
            print(f"  æœ€å°å€¼:      {token_lengths.min():>15,}")
            print(f"  æœ€å¤§å€¼:      {token_lengths.max():>15,}")
        
        print("=" * 60)
    
    def save_statistics(self, output_path: str):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ° JSON æ–‡ä»¶"""
        stats_copy = self.stats.copy()
        
        # è½¬æ¢ numpy æ•°ç»„ä¸ºåˆ—è¡¨ï¼ˆç”¨äº JSON åºåˆ—åŒ–ï¼‰
        if stats_copy['text_lengths']:
            text_lengths = np.array(stats_copy['text_lengths'])
            stats_copy['text_length_stats'] = {
                'mean': float(text_lengths.mean()),
                'median': float(np.median(text_lengths)),
                'min': int(text_lengths.min()),
                'max': int(text_lengths.max()),
                'std': float(text_lengths.std())
            }
            del stats_copy['text_lengths']
        
        if stats_copy['token_lengths']:
            token_lengths = np.array(stats_copy['token_lengths'])
            stats_copy['token_length_stats'] = {
                'mean': float(token_lengths.mean()),
                'median': float(np.median(token_lengths)),
                'min': int(token_lengths.min()),
                'max': int(token_lengths.max()),
                'std': float(token_lengths.std())
            }
            del stats_copy['token_lengths']
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(stats_copy, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    
    # é…ç½®å‚æ•°
    TOKENIZER_PATH = '/root/autodl-tmp/MateGenConv/model/mateconv_tokenizer'
    INPUT_FILE = './dataset/mobvoi_seq_monkey_general_open_corpus.jsonl'
    CLEANED_FILE = './dataset/mobvoi_seq_monkey_general_open_corpus_cleaned.jsonl'
    OUTPUT_FILE = './dataset/pretrain_data_optimized.bin'
    STATS_FILE = './dataset/data_statistics.json'
    
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    processor = OptimizedDataProcessor(
        tokenizer_path=TOKENIZER_PATH,
        max_length=512,
        chunk_size=50000,
        save_interval=1000000,
        num_workers=4
    )
    
    # Step 1: éªŒè¯å’Œæ¸…æ´—æ•°æ®
    if not os.path.exists(CLEANED_FILE):
        valid, invalid = processor.validate_and_clean_jsonl(INPUT_FILE, CLEANED_FILE)
    else:
        print(f"âœ… æ¸…æ´—åçš„æ–‡ä»¶å·²å­˜åœ¨: {CLEANED_FILE}")
    
    # Step 2: å¤„ç†æ•°æ®é›†
    processor.process_dataset(
        input_path=CLEANED_FILE,
        output_path=OUTPUT_FILE,
        resume=True  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    )
    
    # Step 3: ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    processor.save_statistics(STATS_FILE)
    
    print("\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()

