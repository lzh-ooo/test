"""
ä¼˜åŒ–çš„æ¨ç†è„šæœ¬
æ”¹è¿›ç‚¹ï¼š
1. æ”¯æŒå¤šç§é‡‡æ ·ç­–ç•¥ï¼ˆè´ªå¿ƒã€Top-Kã€Top-Pã€Temperatureï¼‰
2. å®ç° KV Cache åŠ é€Ÿç”Ÿæˆ
3. æ”¯æŒæ‰¹é‡ç”Ÿæˆ
4. æ·»åŠ é‡å¤æƒ©ç½šå’Œé•¿åº¦æƒ©ç½š
5. æµå¼è¾“å‡º
6. æ”¯æŒå¤šè½®å¯¹è¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Callable, Dict, Any
from dataclasses import dataclass
import numpy as np

# å‡è®¾è¿™äº›æ¨¡å—å­˜åœ¨
# from model.model import Transformer
# from model.LMConfig import LMConfig
from transformers import AutoTokenizer


@dataclass
class GenerationConfig:
    """ç”Ÿæˆé…ç½®"""
    # é‡‡æ ·ç­–ç•¥
    do_sample: bool = True
    temperature: float = 0.8
    top_k: int = 50
    top_p: float = 0.9
    
    # æƒ©ç½š
    repetition_penalty: float = 1.0
    length_penalty: float = 1.0
    no_repeat_ngram_size: int = 0
    
    # ç”Ÿæˆæ§åˆ¶
    max_new_tokens: int = 100
    min_new_tokens: int = 1
    num_beams: int = 1
    
    # åœæ­¢æ¡ä»¶
    eos_token_id: Optional[int] = None
    pad_token_id: Optional[int] = None
    
    # å…¶ä»–
    use_cache: bool = True
    output_scores: bool = False


class TextGenerator:
    """æ–‡æœ¬ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        model: nn.Module,
        tokenizer: AutoTokenizer,
        device: str = "cuda" if torch.cuda.is_available() else "cpu"
    ):
        """
        Args:
            model: è¯­è¨€æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = torch.device(device)
        self.model.to(self.device)
        self.model.eval()
        
        # ç‰¹æ®Š token IDs
        self.bos_token_id = tokenizer.bos_token_id
        self.eos_token_id = tokenizer.eos_token_id
        self.pad_token_id = tokenizer.pad_token_id
    
    @torch.no_grad()
    def generate(
        self,
        prompt: str,
        config: Optional[GenerationConfig] = None,
        stream: bool = False,
        callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤º
            config: ç”Ÿæˆé…ç½®
            stream: æ˜¯å¦æµå¼è¾“å‡º
            callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if config is None:
            config = GenerationConfig()
        
        # è®¾ç½®ç‰¹æ®Š token
        if config.eos_token_id is None:
            config.eos_token_id = self.eos_token_id
        if config.pad_token_id is None:
            config.pad_token_id = self.pad_token_id
        
        # ç¼–ç è¾“å…¥
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
        
        # ç”Ÿæˆ
        if stream:
            return self._generate_stream(input_ids, config, callback)
        else:
            return self._generate_standard(input_ids, config)
    
    def _generate_standard(
        self,
        input_ids: torch.Tensor,
        config: GenerationConfig
    ) -> str:
        """æ ‡å‡†ç”Ÿæˆï¼ˆéæµå¼ï¼‰"""
        generated_ids = []
        past_key_values = None
        
        # ç”¨äºé‡å¤æƒ©ç½šçš„ ngram å­—å…¸
        ngram_dict = {}
        
        for step in range(config.max_new_tokens):
            # å‰å‘ä¼ æ’­
            if config.use_cache and past_key_values is not None:
                # ä½¿ç”¨ KV Cacheï¼Œåªéœ€ä¼ å…¥æœ€åä¸€ä¸ª token
                outputs = self.model(
                    input_ids[:, -1:],
                    past_key_values=past_key_values,
                    use_cache=True
                )
            else:
                outputs = self.model(input_ids, use_cache=config.use_cache)
            
            logits = outputs.logits[:, -1, :]  # [batch_size, vocab_size]
            past_key_values = outputs.past_key_values if config.use_cache else None
            
            # åº”ç”¨é‡å¤æƒ©ç½š
            if config.repetition_penalty != 1.0:
                logits = self._apply_repetition_penalty(
                    logits,
                    input_ids,
                    config.repetition_penalty
                )
            
            # åº”ç”¨ ngram é˜»æ­¢
            if config.no_repeat_ngram_size > 0:
                logits = self._apply_no_repeat_ngram(
                    logits,
                    generated_ids,
                    config.no_repeat_ngram_size,
                    ngram_dict
                )
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ª token
            next_token_id = self._sample_next_token(logits, config)
            
            # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
            generated_ids.append(next_token_id.item())
            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if next_token_id.item() == config.eos_token_id:
                break
            
            if step + 1 >= config.min_new_tokens and self._should_stop(generated_ids, config):
                break
        
        # è§£ç 
        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        return generated_text
    
    def _generate_stream(
        self,
        input_ids: torch.Tensor,
        config: GenerationConfig,
        callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """æµå¼ç”Ÿæˆ"""
        generated_ids = []
        past_key_values = None
        ngram_dict = {}
        
        for step in range(config.max_new_tokens):
            # å‰å‘ä¼ æ’­
            if config.use_cache and past_key_values is not None:
                outputs = self.model(
                    input_ids[:, -1:],
                    past_key_values=past_key_values,
                    use_cache=True
                )
            else:
                outputs = self.model(input_ids, use_cache=config.use_cache)
            
            logits = outputs.logits[:, -1, :]
            past_key_values = outputs.past_key_values if config.use_cache else None
            
            # åº”ç”¨æƒ©ç½š
            if config.repetition_penalty != 1.0:
                logits = self._apply_repetition_penalty(
                    logits,
                    input_ids,
                    config.repetition_penalty
                )
            
            if config.no_repeat_ngram_size > 0:
                logits = self._apply_no_repeat_ngram(
                    logits,
                    generated_ids,
                    config.no_repeat_ngram_size,
                    ngram_dict
                )
            
            # é‡‡æ ·
            next_token_id = self._sample_next_token(logits, config)
            generated_ids.append(next_token_id.item())
            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)
            
            # æµå¼è¾“å‡º
            if callback is not None:
                token_text = self.tokenizer.decode([next_token_id.item()])
                callback(token_text)
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if next_token_id.item() == config.eos_token_id:
                break
            
            if step + 1 >= config.min_new_tokens and self._should_stop(generated_ids, config):
                break
        
        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        return generated_text
    
    def _sample_next_token(
        self,
        logits: torch.Tensor,
        config: GenerationConfig
    ) -> torch.Tensor:
        """
        é‡‡æ ·ä¸‹ä¸€ä¸ª token
        
        Args:
            logits: [batch_size, vocab_size]
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            next_token_id: [batch_size]
        """
        if not config.do_sample:
            # è´ªå¿ƒè§£ç 
            next_token_id = torch.argmax(logits, dim=-1)
        else:
            # æ¸©åº¦ç¼©æ”¾
            if config.temperature != 1.0:
                logits = logits / config.temperature
            
            # Top-K è¿‡æ»¤
            if config.top_k > 0:
                logits = self._top_k_filtering(logits, config.top_k)
            
            # Top-P (nucleus) è¿‡æ»¤
            if config.top_p < 1.0:
                logits = self._top_p_filtering(logits, config.top_p)
            
            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1).squeeze(1)
        
        return next_token_id
    
    @staticmethod
    def _top_k_filtering(logits: torch.Tensor, top_k: int) -> torch.Tensor:
        """Top-K è¿‡æ»¤"""
        top_k = min(top_k, logits.size(-1))
        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
        logits[indices_to_remove] = float('-inf')
        return logits
    
    @staticmethod
    def _top_p_filtering(logits: torch.Tensor, top_p: float) -> torch.Tensor:
        """Top-P (nucleus) è¿‡æ»¤"""
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ tokens
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        # å°†è¢«ç§»é™¤çš„ tokens çš„ logits è®¾ä¸º -inf
        indices_to_remove = sorted_indices_to_remove.scatter(
            1, sorted_indices, sorted_indices_to_remove
        )
        logits[indices_to_remove] = float('-inf')
        return logits
    
    @staticmethod
    def _apply_repetition_penalty(
        logits: torch.Tensor,
        input_ids: torch.Tensor,
        penalty: float
    ) -> torch.Tensor:
        """åº”ç”¨é‡å¤æƒ©ç½š"""
        batch_size, vocab_size = logits.shape
        
        for i in range(batch_size):
            for token_id in set(input_ids[i].tolist()):
                # å¦‚æœ logit > 0ï¼Œé™¤ä»¥ penaltyï¼›å¦åˆ™ä¹˜ä»¥ penalty
                if logits[i, token_id] < 0:
                    logits[i, token_id] *= penalty
                else:
                    logits[i, token_id] /= penalty
        
        return logits
    
    @staticmethod
    def _apply_no_repeat_ngram(
        logits: torch.Tensor,
        generated_ids: List[int],
        ngram_size: int,
        ngram_dict: Dict
    ) -> torch.Tensor:
        """é˜»æ­¢é‡å¤ n-gram"""
        if len(generated_ids) < ngram_size:
            return logits
        
        # è·å–æœ€è¿‘çš„ n-1 gram
        prefix = tuple(generated_ids[-(ngram_size-1):])
        
        # å¦‚æœè¿™ä¸ª prefix å·²ç»å‡ºç°è¿‡ï¼Œé˜»æ­¢ç›¸åŒçš„ä¸‹ä¸€ä¸ª token
        if prefix in ngram_dict:
            banned_tokens = ngram_dict[prefix]
            logits[0, banned_tokens] = float('-inf')
        
        # æ›´æ–° ngram å­—å…¸
        if len(generated_ids) >= ngram_size:
            ngram_prefix = tuple(generated_ids[-ngram_size:-1])
            next_token = generated_ids[-1]
            if ngram_prefix not in ngram_dict:
                ngram_dict[ngram_prefix] = []
            ngram_dict[ngram_prefix].append(next_token)
        
        return logits
    
    @staticmethod
    def _should_stop(generated_ids: List[int], config: GenerationConfig) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢ç”Ÿæˆ"""
        # å¯ä»¥æ·»åŠ æ›´å¤šåœæ­¢æ¡ä»¶
        return False
    
    def batch_generate(
        self,
        prompts: List[str],
        config: Optional[GenerationConfig] = None
    ) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆ
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        if config is None:
            config = GenerationConfig()
        
        # ç¼–ç è¾“å…¥
        encoded = self.tokenizer(
            prompts,
            padding=True,
            return_tensors='pt'
        )
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        batch_size = input_ids.size(0)
        generated_ids_list = [[] for _ in range(batch_size)]
        
        # æ‰¹é‡ç”Ÿæˆ
        for step in range(config.max_new_tokens):
            outputs = self.model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits[:, -1, :]  # [batch_size, vocab_size]
            
            # å¯¹æ¯ä¸ªæ ·æœ¬åº”ç”¨æƒ©ç½šå’Œé‡‡æ ·
            next_token_ids = []
            for i in range(batch_size):
                sample_logits = logits[i:i+1]
                
                if config.repetition_penalty != 1.0:
                    sample_logits = self._apply_repetition_penalty(
                        sample_logits,
                        input_ids[i:i+1],
                        config.repetition_penalty
                    )
                
                next_token_id = self._sample_next_token(sample_logits, config)
                next_token_ids.append(next_token_id)
                generated_ids_list[i].append(next_token_id.item())
            
            next_token_ids = torch.cat(next_token_ids, dim=0).unsqueeze(1)
            input_ids = torch.cat([input_ids, next_token_ids], dim=1)
            
            # æ›´æ–° attention mask
            attention_mask = torch.cat([
                attention_mask,
                torch.ones((batch_size, 1), device=self.device)
            ], dim=1)
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬éƒ½å·²å®Œæˆ
            if all(
                config.eos_token_id in ids
                for ids in generated_ids_list
            ):
                break
        
        # è§£ç 
        generated_texts = [
            self.tokenizer.decode(ids, skip_special_tokens=True)
            for ids in generated_ids_list
        ]
        
        return generated_texts
    
    def chat(
        self,
        history: List[Dict[str, str]],
        config: Optional[GenerationConfig] = None
    ) -> str:
        """
        å¤šè½®å¯¹è¯
        
        Args:
            history: å¯¹è¯å†å² [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}, ...]
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            åŠ©æ‰‹çš„å›å¤
        """
        # æ„å»ºæç¤º
        prompt = self._build_chat_prompt(history)
        
        # ç”Ÿæˆå›å¤
        response = self.generate(prompt, config)
        
        return response
    
    def _build_chat_prompt(self, history: List[Dict[str, str]]) -> str:
        """æ„å»ºå¯¹è¯æç¤º"""
        prompt_parts = []
        
        for turn in history:
            role = turn['role']
            content = turn['content']
            
            if role == 'user':
                prompt_parts.append(f"ç”¨æˆ·: {content}")
            elif role == 'assistant':
                prompt_parts.append(f"åŠ©æ‰‹: {content}")
        
        prompt_parts.append("åŠ©æ‰‹: ")
        
        return "\n".join(prompt_parts)


def demo():
    """æ¼”ç¤ºç”¨æ³•"""
    print("=" * 60)
    print("æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º")
    print("=" * 60)
    
    # é…ç½®
    TOKENIZER_PATH = '/root/autodl-tmp/MateGenConv/model/mateconv_tokenizer'
    MODEL_PATH = 'out/pretrain_512.pth'
    
    # åŠ è½½åˆ†è¯å™¨
    print("\nğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)
    
    # TODO: åŠ è½½æ¨¡å‹
    # from model.model import Transformer
    # from model.LMConfig import LMConfig
    # lm_config = LMConfig(dim=512, n_layers=8, n_heads=16, vocab_size=6400)
    # model = Transformer(lm_config)
    # model.load_state_dict(torch.load(MODEL_PATH))
    
    # åˆ›å»ºç”Ÿæˆå™¨
    # generator = TextGenerator(model, tokenizer)
    
    # ç”Ÿæˆé…ç½®
    gen_config = GenerationConfig(
        do_sample=True,
        temperature=0.8,
        top_k=50,
        top_p=0.9,
        max_new_tokens=100,
        repetition_penalty=1.2
    )
    
    # ç¤ºä¾‹ 1: æ ‡å‡†ç”Ÿæˆ
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 1: æ ‡å‡†ç”Ÿæˆ")
    print("=" * 60)
    prompt = "ä¸­å›½"
    print(f"è¾“å…¥: {prompt}")
    # result = generator.generate(prompt, gen_config)
    # print(f"è¾“å‡º: {result}")
    
    # ç¤ºä¾‹ 2: æµå¼ç”Ÿæˆ
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 2: æµå¼ç”Ÿæˆ")
    print("=" * 60)
    prompt = "é•¿æ±Ÿã€"
    print(f"è¾“å…¥: {prompt}")
    print("è¾“å‡º: ", end="", flush=True)
    
    def stream_callback(token: str):
        print(token, end="", flush=True)
    
    # result = generator.generate(prompt, gen_config, stream=True, callback=stream_callback)
    print()
    
    # ç¤ºä¾‹ 3: æ‰¹é‡ç”Ÿæˆ
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 3: æ‰¹é‡ç”Ÿæˆ")
    print("=" * 60)
    prompts = ["ä¸­å›½", "é•¿æ±Ÿã€", "ä½ å¥½ï¼Œ"]
    print(f"è¾“å…¥: {prompts}")
    # results = generator.batch_generate(prompts, gen_config)
    # for i, result in enumerate(results):
    #     print(f"è¾“å‡º {i+1}: {result}")
    
    # ç¤ºä¾‹ 4: å¤šè½®å¯¹è¯
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 4: å¤šè½®å¯¹è¯")
    print("=" * 60)
    history = [
        {"role": "user", "content": "ä½ å¥½"},
        {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"},
        {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ä¸­å›½"}
    ]
    # response = generator.chat(history, gen_config)
    # print(f"åŠ©æ‰‹: {response}")


if __name__ == "__main__":
    demo()

