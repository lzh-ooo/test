"""
ä¼˜åŒ–çš„é¢„è®­ç»ƒè„šæœ¬
æ”¹è¿›ç‚¹ï¼š
1. æ·»åŠ å¤šç§ä¼˜åŒ–å™¨é€‰æ‹©ï¼ˆAdamW, Lion, AdaFactorï¼‰
2. æ”¹è¿›å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ”¯æŒçº¿æ€§ã€ä½™å¼¦ã€å¤šé¡¹å¼ï¼‰
3. æ·»åŠ éªŒè¯é›†å’Œæ—©åœæœºåˆ¶
4. æ·»åŠ æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
5. æ·»åŠ æ¨¡å‹ EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
6. æ”¹è¿›æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤
7. æ›´è¯¦ç»†çš„æ—¥å¿—å’Œå¯è§†åŒ–
8. æ”¯æŒå¤šç§ç²¾åº¦è®­ç»ƒï¼ˆfp16, bf16, fp32ï¼‰
"""

import os
import platform
import argparse
import time
import math
import warnings
import json
from typing import Optional, Dict, Any
from dataclasses import dataclass, asdict

import torch
import torch.nn as nn
import torch.distributed as dist
from torch import optim
from torch.nn.parallel import DistributedDataParallel
from torch.optim.lr_scheduler import (
    CosineAnnealingLR, 
    LinearLR, 
    PolynomialLR,
    SequentialLR
)
from torch.utils.data import DataLoader, DistributedSampler
from contextlib import nullcontext

# å‡è®¾è¿™äº›æ¨¡å—å­˜åœ¨
# from model.model import Transformer
# from model.LMConfig import LMConfig
# from model.dataset import PretrainDataset

warnings.filterwarnings('ignore')


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # åŸºç¡€é…ç½®
    out_dir: str = "out"
    data_path: str = "./dataset/pretrain_data.bin"
    val_data_path: Optional[str] = None  # éªŒè¯é›†è·¯å¾„
    
    # è®­ç»ƒè¶…å‚æ•°
    epochs: int = 15
    batch_size: int = 32
    eval_batch_size: int = 64
    learning_rate: float = 3e-4
    min_lr: float = 3e-5
    weight_decay: float = 0.1
    beta1: float = 0.9
    beta2: float = 0.95
    
    # å­¦ä¹ ç‡è°ƒåº¦
    lr_scheduler: str = "cosine"  # cosine, linear, polynomial
    warmup_iters: int = 2000
    warmup_ratio: float = 0.1  # warmup æ­¥æ•°å æ€»æ­¥æ•°çš„æ¯”ä¾‹
    
    # ä¼˜åŒ–å™¨é€‰æ‹©
    optimizer_type: str = "adamw"  # adamw, adam, lion, adafactor
    
    # æ­£åˆ™åŒ–
    grad_clip: float = 1.0
    dropout: float = 0.1
    
    # æ¢¯åº¦ç´¯ç§¯
    accumulation_steps: int = 1
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    dtype: str = "bfloat16"  # float32, float16, bfloat16
    
    # EMA
    use_ema: bool = False
    ema_decay: float = 0.9999
    
    # éªŒè¯å’Œä¿å­˜
    eval_interval: int = 500
    eval_iters: int = 100
    save_interval: int = 1000
    log_interval: int = 10
    
    # æ—©åœ
    early_stopping: bool = False
    patience: int = 5
    
    # åˆ†å¸ƒå¼è®­ç»ƒ
    ddp: bool = False
    local_rank: int = -1
    
    # è®¾å¤‡
    device: str = "cuda:0" if torch.cuda.is_available() else "cpu"
    num_workers: int = 4
    
    # Wandb
    use_wandb: bool = False
    wandb_project: str = "MateConv-Pretrain"
    wandb_run_name: Optional[str] = None
    
    # æ¢å¤è®­ç»ƒ
    resume: bool = False
    resume_from: Optional[str] = None
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.wandb_run_name is None:
            self.wandb_run_name = f"E{self.epochs}-BS{self.batch_size}-LR{self.learning_rate}"


class ModelEMA:
    """æ¨¡å‹æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
    
    def __init__(self, model: nn.Module, decay: float = 0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        # æ³¨å†Œå‚æ•°
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        """æ›´æ–° EMA å‚æ•°"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()
    
    def apply_shadow(self):
        """åº”ç”¨ EMA å‚æ•°"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]
    
    def restore(self):
        """æ¢å¤åŸå§‹å‚æ•°"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}


class Trainer:
    """è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = torch.device(config.device)
        self.ddp = config.ddp
        self.ddp_local_rank = 0
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼
        if self.ddp:
            self._init_distributed()
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(1337)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(1337)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config.out_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        self.ema = None
        self.train_loader = None
        self.val_loader = None
        self.wandb = None
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
        # è®¾ç½®æ··åˆç²¾åº¦
        self.ctx = self._get_autocast_context()
        
        # åˆå§‹åŒ– wandb
        if config.use_wandb and (not self.ddp or self.ddp_local_rank == 0):
            import wandb
            self.wandb = wandb
            wandb.init(
                project=config.wandb_project,
                name=config.wandb_run_name,
                config=asdict(config)
            )
    
    def _init_distributed(self):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
        dist.init_process_group(backend="nccl")
        self.ddp_local_rank = int(os.environ["LOCAL_RANK"])
        self.device = torch.device(f"cuda:{self.ddp_local_rank}")
        torch.cuda.set_device(self.device)
    
    def _get_autocast_context(self):
        """è·å–è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡"""
        if "cuda" not in str(self.device):
            return nullcontext()
        
        dtype_map = {
            "float32": torch.float32,
            "float16": torch.float16,
            "bfloat16": torch.bfloat16
        }
        dtype = dtype_map.get(self.config.dtype, torch.bfloat16)
        return torch.cuda.amp.autocast(dtype=dtype)
    
    def _log(self, message: str):
        """æ‰“å°æ—¥å¿—ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰"""
        if not self.ddp or dist.get_rank() == 0:
            print(message)
    
    def setup_model(self, model: nn.Module):
        """è®¾ç½®æ¨¡å‹"""
        self.model = model.to(self.device)
        
        # ç»Ÿè®¡å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self._log(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params / 1e6:.2f}M (å¯è®­ç»ƒ: {trainable_params / 1e6:.2f}M)")
        
        # åˆ†å¸ƒå¼åŒ…è£…
        if self.ddp:
            self.model = DistributedDataParallel(
                self.model,
                device_ids=[self.ddp_local_rank]
            )
        
        # EMA
        if self.config.use_ema:
            self.ema = ModelEMA(self.model, decay=self.config.ema_decay)
            self._log(f"âœ… å¯ç”¨ EMA (decay={self.config.ema_decay})")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        config = self.config
        
        # å‚æ•°åˆ†ç»„ï¼ˆå¯¹ä¸åŒå‚æ•°åº”ç”¨ä¸åŒçš„ weight decayï¼‰
        decay_params = []
        nodecay_params = []
        
        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            
            # ä¸å¯¹ bias å’Œ LayerNorm å‚æ•°åº”ç”¨ weight decay
            if param.ndim < 2 or 'bias' in name or 'norm' in name:
                nodecay_params.append(param)
            else:
                decay_params.append(param)
        
        optim_groups = [
            {'params': decay_params, 'weight_decay': config.weight_decay},
            {'params': nodecay_params, 'weight_decay': 0.0}
        ]
        
        # é€‰æ‹©ä¼˜åŒ–å™¨
        if config.optimizer_type == "adamw":
            self.optimizer = optim.AdamW(
                optim_groups,
                lr=config.learning_rate,
                betas=(config.beta1, config.beta2)
            )
        elif config.optimizer_type == "adam":
            self.optimizer = optim.Adam(
                optim_groups,
                lr=config.learning_rate,
                betas=(config.beta1, config.beta2)
            )
        else:
            # é»˜è®¤ä½¿ç”¨ AdamW
            self.optimizer = optim.AdamW(
                optim_groups,
                lr=config.learning_rate,
                betas=(config.beta1, config.beta2)
            )
        
        self._log(f"âœ… ä¼˜åŒ–å™¨: {config.optimizer_type.upper()}")
        self._log(f"   - å­¦ä¹ ç‡: {config.learning_rate}")
        self._log(f"   - Weight Decay: {config.weight_decay}")
        
        # æ¢¯åº¦ç¼©æ”¾å™¨
        use_amp = config.dtype in ['float16', 'bfloat16']
        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
    
    def setup_scheduler(self, steps_per_epoch: int):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        config = self.config
        total_steps = steps_per_epoch * config.epochs
        warmup_steps = int(total_steps * config.warmup_ratio) if config.warmup_ratio > 0 else config.warmup_iters
        
        # Warmup è°ƒåº¦å™¨
        warmup_scheduler = LinearLR(
            self.optimizer,
            start_factor=0.01,
            end_factor=1.0,
            total_iters=warmup_steps
        )
        
        # ä¸»è°ƒåº¦å™¨
        if config.lr_scheduler == "cosine":
            main_scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=total_steps - warmup_steps,
                eta_min=config.min_lr
            )
        elif config.lr_scheduler == "linear":
            main_scheduler = LinearLR(
                self.optimizer,
                start_factor=1.0,
                end_factor=config.min_lr / config.learning_rate,
                total_iters=total_steps - warmup_steps
            )
        else:
            main_scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=total_steps - warmup_steps,
                eta_min=config.min_lr
            )
        
        # ç»„åˆè°ƒåº¦å™¨
        self.scheduler = SequentialLR(
            self.optimizer,
            schedulers=[warmup_scheduler, main_scheduler],
            milestones=[warmup_steps]
        )
        
        self._log(f"âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨: {config.lr_scheduler}")
        self._log(f"   - Warmup æ­¥æ•°: {warmup_steps}")
        self._log(f"   - æ€»æ­¥æ•°: {total_steps}")
    
    def setup_dataloaders(self, train_dataset, val_dataset=None):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        train_sampler = DistributedSampler(train_dataset) if self.ddp else None
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            sampler=train_sampler,
            shuffle=(train_sampler is None),
            num_workers=self.config.num_workers,
            pin_memory=True,
            drop_last=True
        )
        
        if val_dataset is not None:
            val_sampler = DistributedSampler(val_dataset, shuffle=False) if self.ddp else None
            self.val_loader = DataLoader(
                val_dataset,
                batch_size=self.config.eval_batch_size,
                sampler=val_sampler,
                shuffle=False,
                num_workers=self.config.num_workers,
                pin_memory=True,
                drop_last=False
            )
        
        self._log(f"âœ… æ•°æ®åŠ è½½å™¨è®¾ç½®å®Œæˆ")
        self._log(f"   - è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset):,}")
        if val_dataset:
            self._log(f"   - éªŒè¯æ ·æœ¬æ•°: {len(val_dataset):,}")
    
    def train_step(self, batch) -> float:
        """å•æ­¥è®­ç»ƒ"""
        X, Y = batch
        X = X.to(self.device)
        Y = Y.to(self.device)
        
        # å‰å‘ä¼ æ’­
        with self.ctx:
            output = self.model(X, Y)
            loss = output.last_loss / self.config.accumulation_steps
        
        # åå‘ä¼ æ’­
        self.scaler.scale(loss).backward()
        
        # æ¢¯åº¦ç´¯ç§¯
        if (self.global_step + 1) % self.config.accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            self.scaler.unscale_(self.optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.config.grad_clip
            )
            
            # ä¼˜åŒ–å™¨æ­¥è¿›
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.optimizer.zero_grad(set_to_none=True)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # EMA æ›´æ–°
            if self.ema is not None:
                self.ema.update()
            
            return loss.item() * self.config.accumulation_steps, grad_norm.item()
        
        return loss.item() * self.config.accumulation_steps, 0.0
    
    @torch.no_grad()
    def evaluate(self) -> float:
        """è¯„ä¼°æ¨¡å‹"""
        if self.val_loader is None:
            return 0.0
        
        self.model.eval()
        
        # å¦‚æœä½¿ç”¨ EMAï¼Œåº”ç”¨ shadow å‚æ•°
        if self.ema is not None:
            self.ema.apply_shadow()
        
        total_loss = 0.0
        num_batches = 0
        
        for batch in self.val_loader:
            X, Y = batch
            X = X.to(self.device)
            Y = Y.to(self.device)
            
            with self.ctx:
                output = self.model(X, Y)
                loss = output.last_loss
            
            total_loss += loss.item()
            num_batches += 1
            
            if num_batches >= self.config.eval_iters:
                break
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        
        # æ¢å¤åŸå§‹å‚æ•°
        if self.ema is not None:
            self.ema.restore()
        
        self.model.train()
        
        return avg_loss
    
    def save_checkpoint(self, filename: str, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if self.ddp and dist.get_rank() != 0:
            return
        
        # è·å–æ¨¡å‹çŠ¶æ€
        if isinstance(self.model, DistributedDataParallel):
            model_state = self.model.module.state_dict()
        else:
            model_state = self.model.state_dict()
        
        checkpoint = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'model_state_dict': model_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': asdict(self.config)
        }
        
        if self.ema is not None:
            checkpoint['ema_shadow'] = self.ema.shadow
        
        filepath = os.path.join(self.config.out_dir, filename)
        torch.save(checkpoint, filepath)
        self._log(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
        
        if is_best:
            best_path = os.path.join(self.config.out_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            self._log(f"ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")
    
    def load_checkpoint(self, filepath: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        self._log(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {filepath}")
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # åŠ è½½æ¨¡å‹
        if isinstance(self.model, DistributedDataParallel):
            self.model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # åŠ è½½ä¼˜åŒ–å™¨
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # åŠ è½½è°ƒåº¦å™¨
        if 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # åŠ è½½è®­ç»ƒçŠ¶æ€
        self.current_epoch = checkpoint.get('epoch', 0)
        self.global_step = checkpoint.get('global_step', 0)
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        # åŠ è½½ EMA
        if self.ema is not None and 'ema_shadow' in checkpoint:
            self.ema.shadow = checkpoint['ema_shadow']
        
        self._log(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ (epoch={self.current_epoch}, step={self.global_step})")
    
    def train(self):
        """è®­ç»ƒä¸»å¾ªç¯"""
        self._log("\n" + "=" * 60)
        self._log("ğŸš€ å¼€å§‹è®­ç»ƒ")
        self._log("=" * 60)
        
        steps_per_epoch = len(self.train_loader)
        self.setup_scheduler(steps_per_epoch)
        
        # æ¢å¤è®­ç»ƒ
        if self.config.resume and self.config.resume_from:
            self.load_checkpoint(self.config.resume_from)
        
        start_time = time.time()
        
        for epoch in range(self.current_epoch, self.config.epochs):
            self.current_epoch = epoch
            epoch_start_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ª epoch
            self.model.train()
            epoch_loss = 0.0
            
            for step, batch in enumerate(self.train_loader):
                loss, grad_norm = self.train_step(batch)
                epoch_loss += loss
                self.global_step += 1
                
                # æ—¥å¿—è®°å½•
                if self.global_step % self.config.log_interval == 0:
                    lr = self.optimizer.param_groups[0]['lr']
                    elapsed = time.time() - start_time
                    
                    log_msg = (
                        f"Epoch [{epoch+1}/{self.config.epochs}] "
                        f"Step [{step+1}/{steps_per_epoch}] "
                        f"Loss: {loss:.4f} | "
                        f"LR: {lr:.2e} | "
                        f"GradNorm: {grad_norm:.2f}"
                    )
                    self._log(log_msg)
                    
                    if self.wandb is not None:
                        self.wandb.log({
                            'train/loss': loss,
                            'train/lr': lr,
                            'train/grad_norm': grad_norm,
                            'train/epoch': epoch,
                            'train/step': self.global_step
                        })
                
                # éªŒè¯
                if self.val_loader and self.global_step % self.config.eval_interval == 0:
                    val_loss = self.evaluate()
                    self._log(f"ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.4f}")
                    
                    if self.wandb is not None:
                        self.wandb.log({'val/loss': val_loss})
                    
                    # æ—©åœæ£€æŸ¥
                    if val_loss < self.best_val_loss:
                        self.best_val_loss = val_loss
                        self.patience_counter = 0
                        self.save_checkpoint(f'checkpoint_best.pth', is_best=True)
                    else:
                        self.patience_counter += 1
                        
                        if self.config.early_stopping and self.patience_counter >= self.config.patience:
                            self._log(f"âš ï¸  æ—©åœè§¦å‘ (patience={self.config.patience})")
                            return
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if self.global_step % self.config.save_interval == 0:
                    self.save_checkpoint(f'checkpoint_step_{self.global_step}.pth')
            
            # Epoch ç»“æŸ
            avg_epoch_loss = epoch_loss / steps_per_epoch
            epoch_time = time.time() - epoch_start_time
            
            self._log(f"\n{'='*60}")
            self._log(f"Epoch {epoch+1} å®Œæˆ")
            self._log(f"å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}")
            self._log(f"è€—æ—¶: {epoch_time/60:.2f} åˆ†é’Ÿ")
            self._log(f"{'='*60}\n")
            
            # ä¿å­˜ epoch æ£€æŸ¥ç‚¹
            self.save_checkpoint(f'checkpoint_epoch_{epoch+1}.pth')
        
        total_time = time.time() - start_time
        self._log(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–çš„ MateConv é¢„è®­ç»ƒ")
    
    # ä»é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--out_dir", type=str, default="out")
    parser.add_argument("--epochs", type=int, default=15)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--learning_rate", type=float, default=3e-4)
    parser.add_argument("--optimizer_type", type=str, default="adamw")
    parser.add_argument("--lr_scheduler", type=str, default="cosine")
    parser.add_argument("--use_ema", action="store_true")
    parser.add_argument("--early_stopping", action="store_true")
    parser.add_argument("--use_wandb", action="store_true")
    parser.add_argument("--resume", action="store_true")
    parser.add_argument("--resume_from", type=str)
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            config_dict = json.load(f)
        config = TrainingConfig(**config_dict)
    else:
        config = TrainingConfig(
            out_dir=args.out_dir,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            optimizer_type=args.optimizer_type,
            lr_scheduler=args.lr_scheduler,
            use_ema=args.use_ema,
            early_stopping=args.early_stopping,
            use_wandb=args.use_wandb,
            resume=args.resume,
            resume_from=args.resume_from
        )
    
    # ä¿å­˜é…ç½®
    config_path = os.path.join(config.out_dir, 'training_config.json')
    os.makedirs(config.out_dir, exist_ok=True)
    with open(config_path, 'w') as f:
        json.dump(asdict(config), f, indent=2)
    
    print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(config)
    
    # TODO: åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®é›†
    # model = Transformer(lm_config)
    # train_dataset = PretrainDataset(...)
    # val_dataset = PretrainDataset(...)
    
    # trainer.setup_model(model)
    # trainer.setup_optimizer()
    # trainer.setup_dataloaders(train_dataset, val_dataset)
    # trainer.train()


if __name__ == "__main__":
    main()

