"""
å¿«é€Ÿå¯åŠ¨è„šæœ¬ - ç”¨äºæµ‹è¯•ä¼˜åŒ–åçš„è®­ç»ƒå’Œæ¨ç†æµç¨‹
è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¼˜åŒ–åçš„ä»£ç 
"""

import os
import torch
from transformers import AutoTokenizer


def demo_data_processing():
    """æ¼”ç¤ºæ•°æ®å¤„ç†"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ•°æ®å¤„ç†æ¼”ç¤º")
    print("=" * 80)
    
    from optimized_data_processing import OptimizedDataProcessor
    
    # é…ç½®
    TOKENIZER_PATH = '/root/autodl-tmp/MateGenConv/model/mateconv_tokenizer'
    INPUT_FILE = './dataset/mobvoi_seq_monkey_general_open_corpus.jsonl'
    CLEANED_FILE = './dataset/cleaned_data.jsonl'
    OUTPUT_FILE = './dataset/pretrain_data_optimized.bin'
    STATS_FILE = './dataset/data_stats.json'
    
    print("\nåˆ›å»ºæ•°æ®å¤„ç†å™¨...")
    processor = OptimizedDataProcessor(
        tokenizer_path=TOKENIZER_PATH,
        max_length=512,
        chunk_size=50000,
        save_interval=1000000
    )
    
    # Step 1: æ¸…æ´—æ•°æ®
    if not os.path.exists(CLEANED_FILE):
        print("\nğŸ§¹ æ­¥éª¤ 1: æ¸…æ´—æ•°æ®...")
        valid, invalid = processor.validate_and_clean_jsonl(INPUT_FILE, CLEANED_FILE)
        print(f"âœ… æœ‰æ•ˆ: {valid:,}, æ— æ•ˆ: {invalid:,}")
    else:
        print(f"\nâœ… æ¸…æ´—åçš„æ–‡ä»¶å·²å­˜åœ¨: {CLEANED_FILE}")
    
    # Step 2: å¤„ç†æ•°æ®
    print("\nğŸš€ æ­¥éª¤ 2: å¤„ç†æ•°æ®é›†...")
    processor.process_dataset(
        input_path=CLEANED_FILE,
        output_path=OUTPUT_FILE,
        resume=True
    )
    
    # Step 3: ä¿å­˜ç»Ÿè®¡
    print("\nğŸ“Š æ­¥éª¤ 3: ä¿å­˜ç»Ÿè®¡ä¿¡æ¯...")
    processor.save_statistics(STATS_FILE)
    
    print("\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")


def demo_training():
    """æ¼”ç¤ºè®­ç»ƒæµç¨‹"""
    print("\n" + "=" * 80)
    print("ğŸ¯ è®­ç»ƒæ¼”ç¤º")
    print("=" * 80)
    
    from optimized_pretrain import Trainer, TrainingConfig
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = TrainingConfig(
        # æ•°æ®
        data_path="./dataset/pretrain_data.bin",
        val_data_path=None,
        
        # è®­ç»ƒå‚æ•°
        epochs=3,  # æ¼”ç¤ºç”¨ï¼Œåªè®­ç»ƒ3ä¸ªepoch
        batch_size=16,  # è¾ƒå°çš„batch size
        learning_rate=3e-4,
        min_lr=3e-5,
        
        # ä¼˜åŒ–å™¨
        optimizer_type="adamw",
        lr_scheduler="cosine",
        warmup_ratio=0.1,
        
        # è®­ç»ƒæŠ€å·§
        use_ema=True,
        grad_clip=1.0,
        accumulation_steps=2,
        dtype="bfloat16",
        
        # éªŒè¯å’Œä¿å­˜
        eval_interval=500,
        save_interval=1000,
        log_interval=10,
        
        # è¾“å‡º
        out_dir="out_demo",
        
        # Wandb
        use_wandb=False
    )
    
    print("\né…ç½®:")
    print(f"  Epochs: {config.epochs}")
    print(f"  Batch Size: {config.batch_size}")
    print(f"  Learning Rate: {config.learning_rate}")
    print(f"  Optimizer: {config.optimizer_type}")
    print(f"  Use EMA: {config.use_ema}")
    
    # TODO: åŠ è½½å®é™…çš„æ¨¡å‹å’Œæ•°æ®
    # from model.model import Transformer
    # from model.LMConfig import LMConfig
    # from model.dataset import PretrainDataset
    
    print("\nâš ï¸  æ³¨æ„: éœ€è¦å®é™…çš„æ¨¡å‹ä»£ç æ‰èƒ½è¿è¡Œè®­ç»ƒ")
    print("   è¯·ç¡®ä¿ model/ ç›®å½•ä¸­åŒ…å«:")
    print("   - model.py (Transformer æ¨¡å‹)")
    print("   - LMConfig.py (æ¨¡å‹é…ç½®)")
    print("   - dataset.py (æ•°æ®é›†ç±»)")


def demo_inference():
    """æ¼”ç¤ºæ¨ç†æµç¨‹"""
    print("\n" + "=" * 80)
    print("ğŸ’¬ æ¨ç†æ¼”ç¤º")
    print("=" * 80)
    
    from optimized_inference import TextGenerator, GenerationConfig
    
    # é…ç½®
    TOKENIZER_PATH = '/root/autodl-tmp/MateGenConv/model/mateconv_tokenizer'
    MODEL_PATH = 'out/pretrain_512.pth'
    
    if not os.path.exists(MODEL_PATH):
        print(f"\nâš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
        print("   è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æä¾›é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
        return
    
    print("\nğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)
        print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ (è¯è¡¨å¤§å°: {len(tokenizer)})")
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        return
    
    # TODO: åŠ è½½æ¨¡å‹
    print("\nâš ï¸  æ³¨æ„: éœ€è¦å®é™…çš„æ¨¡å‹ä»£ç æ‰èƒ½è¿è¡Œæ¨ç†")
    print("   è¯·ç¡®ä¿ model/ ç›®å½•ä¸­åŒ…å«:")
    print("   - model.py (Transformer æ¨¡å‹)")
    print("   - LMConfig.py (æ¨¡å‹é…ç½®)")
    
    # æ¼”ç¤ºé…ç½®
    gen_config = GenerationConfig(
        do_sample=True,
        temperature=0.8,
        top_k=50,
        top_p=0.9,
        repetition_penalty=1.2,
        max_new_tokens=100
    )
    
    print("\nç”Ÿæˆé…ç½®:")
    print(f"  Temperature: {gen_config.temperature}")
    print(f"  Top-K: {gen_config.top_k}")
    print(f"  Top-P: {gen_config.top_p}")
    print(f"  Repetition Penalty: {gen_config.repetition_penalty}")
    
    print("\nğŸ“ ç¤ºä¾‹æç¤º:")
    prompts = [
        "ä¸­å›½",
        "é•¿æ±Ÿã€é»„æ²³",
        "ä½ å¥½ï¼Œå¥½ä¹…ä¸è§"
    ]
    for i, prompt in enumerate(prompts, 1):
        print(f"  {i}. {prompt}")


def show_command_examples():
    """æ˜¾ç¤ºå‘½ä»¤è¡Œç¤ºä¾‹"""
    print("\n" + "=" * 80)
    print("ğŸ“ å‘½ä»¤è¡Œä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)
    
    print("\nã€æ•°æ®å¤„ç†ã€‘")
    print("python optimized_data_processing.py")
    
    print("\nã€è®­ç»ƒ - ä½¿ç”¨é…ç½®æ–‡ä»¶ã€‘")
    print("python train.py --config config.yaml")
    
    print("\nã€è®­ç»ƒ - ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ã€‘")
    print("python train.py \\")
    print("    --epochs 15 \\")
    print("    --batch_size 32 \\")
    print("    --learning_rate 3e-4 \\")
    print("    --use_ema \\")
    print("    --early_stopping")
    
    print("\nã€è®­ç»ƒ - åˆ†å¸ƒå¼è®­ç»ƒã€‘")
    print("torchrun --nproc_per_node=2 train.py --config config.yaml --ddp")
    
    print("\nã€æ¨ç† - å•æ¬¡ç”Ÿæˆã€‘")
    print("python inference.py \\")
    print("    --model_path out/pretrain_512.pth \\")
    print("    --prompt \"ä¸­å›½\" \\")
    print("    --temperature 0.8")
    
    print("\nã€æ¨ç† - äº¤äº’å¼å¯¹è¯ã€‘")
    print("python inference.py \\")
    print("    --model_path out/pretrain_512.pth \\")
    print("    --interactive")
    
    print("\nã€æ¨ç† - æ‰¹é‡ç”Ÿæˆã€‘")
    print("python inference.py \\")
    print("    --model_path out/pretrain_512.pth \\")
    print("    --batch_file prompts.txt")


def show_optimization_summary():
    """æ˜¾ç¤ºä¼˜åŒ–æ€»ç»“"""
    print("\n" + "=" * 80)
    print("ğŸ‰ ä¼˜åŒ–æ€»ç»“")
    print("=" * 80)
    
    print("\nã€æ•°æ®å¤„ç†ä¼˜åŒ–ã€‘")
    print("  âœ… åˆå¹¶æ¸…æ´—å’Œé¢„å¤„ç†æ­¥éª¤")
    print("  âœ… æ·»åŠ æ–­ç‚¹ç»­ä¼ åŠŸèƒ½")
    print("  âœ… è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡å’Œå¯è§†åŒ–")
    print("  âœ… æ›´å¥½çš„é”™è¯¯å¤„ç†")
    
    print("\nã€è®­ç»ƒä¼˜åŒ–ã€‘")
    print("  âœ… å¤šç§ä¼˜åŒ–å™¨ (AdamW, Adam)")
    print("  âœ… çµæ´»çš„å­¦ä¹ ç‡è°ƒåº¦ (Cosine, Linear, Polynomial)")
    print("  âœ… æ¨¡å‹ EMA æå‡ç”Ÿæˆè´¨é‡")
    print("  âœ… éªŒè¯é›†å’Œæ—©åœæœºåˆ¶")
    print("  âœ… å®Œå–„çš„æ£€æŸ¥ç‚¹ç®¡ç†")
    print("  âœ… Wandb é›†æˆ")
    print("  âœ… åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ")
    
    print("\nã€æ¨ç†ä¼˜åŒ–ã€‘")
    print("  âœ… å¤šç§é‡‡æ ·ç­–ç•¥ (Greedy, Top-K, Top-P)")
    print("  âœ… KV Cache åŠ é€Ÿç”Ÿæˆ")
    print("  âœ… æ‰¹é‡ç”Ÿæˆæ”¯æŒ")
    print("  âœ… æµå¼è¾“å‡º")
    print("  âœ… äº¤äº’å¼å¯¹è¯")
    print("  âœ… é‡å¤æƒ©ç½šå’Œ ngram é˜»æ­¢")
    
    print("\nã€ä»£ç è´¨é‡ã€‘")
    print("  âœ… æ¸…æ™°çš„ä»£ç ç»“æ„")
    print("  âœ… è¯¦ç»†çš„æ³¨é‡Š")
    print("  âœ… é…ç½®æ–‡ä»¶æ”¯æŒ")
    print("  âœ… å®Œå–„çš„é”™è¯¯å¤„ç†")


def main():
    """ä¸»èœå•"""
    print("\n" + "=" * 80)
    print("ğŸš€ MateConv é¢„è®­ç»ƒä¼˜åŒ–ç‰ˆ - å¿«é€Ÿå¯åŠ¨")
    print("=" * 80)
    
    while True:
        print("\nè¯·é€‰æ‹©è¦æ¼”ç¤ºçš„åŠŸèƒ½:")
        print("  1. æ•°æ®å¤„ç†æ¼”ç¤º")
        print("  2. è®­ç»ƒæµç¨‹æ¼”ç¤º")
        print("  3. æ¨ç†æµç¨‹æ¼”ç¤º")
        print("  4. å‘½ä»¤è¡Œç¤ºä¾‹")
        print("  5. ä¼˜åŒ–æ€»ç»“")
        print("  6. å…¨éƒ¨æ¼”ç¤º")
        print("  0. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-6): ").strip()
        
        if choice == "1":
            # demo_data_processing()  # å®é™…è¿è¡Œä¼šå¾ˆæ…¢
            print("\nâš ï¸  æ•°æ®å¤„ç†éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¿™é‡Œåªå±•ç¤ºä»£ç ")
            print("    å®é™…ä½¿ç”¨æ—¶è¯·å–æ¶ˆæ³¨é‡Šè¿è¡Œ")
            
        elif choice == "2":
            demo_training()
            
        elif choice == "3":
            demo_inference()
            
        elif choice == "4":
            show_command_examples()
            
        elif choice == "5":
            show_optimization_summary()
            
        elif choice == "6":
            # demo_data_processing()
            demo_training()
            demo_inference()
            show_command_examples()
            show_optimization_summary()
            
        elif choice == "0":
            print("\nğŸ‘‹ å†è§ï¼")
            break
            
        else:
            print("\nâŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥")


if __name__ == "__main__":
    main()

