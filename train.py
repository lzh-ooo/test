"""
ç®€åŒ–çš„è®­ç»ƒå¯åŠ¨è„šæœ¬
ç”¨æ³•:
    python train.py --config config.yaml
    æˆ–
    python train.py --epochs 15 --batch_size 32 --learning_rate 3e-4
"""

import os
import sys
import argparse
import torch
from pathlib import Path

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆéœ€è¦ç¡®ä¿è¿™äº›æ¨¡å—å­˜åœ¨ï¼‰
try:
    from model.model import Transformer
    from model.LMConfig import LMConfig
    from model.dataset import PretrainDataset
except ImportError:
    print("âš ï¸  è­¦å‘Š: æ— æ³•å¯¼å…¥æ¨¡å‹æ¨¡å—ï¼Œè¯·ç¡®ä¿ model/ ç›®å½•å­˜åœ¨")
    print("   è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è„šæœ¬ï¼Œéœ€è¦æ‚¨çš„å®é™…æ¨¡å‹ä»£ç ")
    # åˆ›å»ºæ¨¡æ‹Ÿç±»ä»¥ä¾¿æ¼”ç¤º
    class LMConfig:
        def __init__(self):
            self.dim = 512
            self.n_layers = 8
            self.n_heads = 16
            self.vocab_size = 6400
            self.max_seq_len = 512
            self.dropout = 0.1
            self.use_moe = False
    
    class Transformer:
        pass
    
    class PretrainDataset:
        pass

from utils import load_config, set_seed, get_device, print_model_structure
from optimized_pretrain import Trainer, TrainingConfig
from transformers import AutoTokenizer


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="MateConv é¢„è®­ç»ƒ")
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="YAML é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    # æ•°æ®ç›¸å…³
    parser.add_argument("--data_path", type=str, default="./dataset/pretrain_data.bin")
    parser.add_argument("--val_data_path", type=str, default=None)
    parser.add_argument("--tokenizer_path", type=str, 
                       default="/root/autodl-tmp/MateGenConv/model/mateconv_tokenizer")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=15)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--learning_rate", type=float, default=3e-4)
    parser.add_argument("--min_lr", type=float, default=3e-5)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    parser.add_argument("--optimizer_type", type=str, default="adamw",
                       choices=["adamw", "adam"])
    parser.add_argument("--lr_scheduler", type=str, default="cosine",
                       choices=["cosine", "linear", "polynomial"])
    parser.add_argument("--warmup_ratio", type=float, default=0.1)
    
    # è®­ç»ƒæŠ€å·§
    parser.add_argument("--use_ema", action="store_true", help="ä½¿ç”¨ EMA")
    parser.add_argument("--grad_clip", type=float, default=1.0)
    parser.add_argument("--accumulation_steps", type=int, default=1)
    parser.add_argument("--dtype", type=str, default="bfloat16",
                       choices=["float32", "float16", "bfloat16"])
    
    # éªŒè¯å’Œä¿å­˜
    parser.add_argument("--eval_interval", type=int, default=500)
    parser.add_argument("--save_interval", type=int, default=1000)
    parser.add_argument("--log_interval", type=int, default=10)
    parser.add_argument("--early_stopping", action="store_true")
    parser.add_argument("--patience", type=int, default=5)
    
    # è¾“å‡ºå’Œæ¢å¤
    parser.add_argument("--out_dir", type=str, default="out")
    parser.add_argument("--resume", action="store_true", help="æ¢å¤è®­ç»ƒ")
    parser.add_argument("--resume_from", type=str, default=None)
    
    # åˆ†å¸ƒå¼
    parser.add_argument("--ddp", action="store_true", help="ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
    parser.add_argument("--local_rank", type=int, default=-1)
    
    # Wandb
    parser.add_argument("--use_wandb", action="store_true")
    parser.add_argument("--wandb_project", type=str, default="MateConv-Pretrain")
    parser.add_argument("--wandb_run_name", type=str, default=None)
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=1337)
    parser.add_argument("--num_workers", type=int, default=4)
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("ğŸš€ MateConv é¢„è®­ç»ƒè„šæœ¬")
    print("=" * 80 + "\n")
    
    # è§£æå‚æ•°
    args = parse_args()
    
    # åŠ è½½é…ç½®ï¼ˆå¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼‰
    if args.config and os.path.exists(args.config):
        print(f"ğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        config_dict = load_config(args.config)
        
        # ä»é…ç½®æ–‡ä»¶åˆ›å»ºè®­ç»ƒé…ç½®
        training_cfg = config_dict.get('training', {})
        data_cfg = config_dict.get('data', {})
        
        config = TrainingConfig(
            # æ•°æ®
            data_path=data_cfg.get('train_data_path', args.data_path),
            val_data_path=data_cfg.get('val_data_path', args.val_data_path),
            
            # è®­ç»ƒå‚æ•°
            epochs=training_cfg.get('epochs', args.epochs),
            batch_size=training_cfg.get('batch_size', args.batch_size),
            learning_rate=training_cfg.get('learning_rate', args.learning_rate),
            min_lr=training_cfg.get('min_lr', args.min_lr),
            weight_decay=training_cfg.get('weight_decay', args.weight_decay),
            
            # ä¼˜åŒ–å™¨
            optimizer_type=training_cfg.get('optimizer_type', args.optimizer_type),
            lr_scheduler=training_cfg.get('lr_scheduler', args.lr_scheduler),
            warmup_ratio=training_cfg.get('warmup_ratio', args.warmup_ratio),
            
            # è®­ç»ƒæŠ€å·§
            use_ema=training_cfg.get('use_ema', args.use_ema),
            grad_clip=training_cfg.get('grad_clip', args.grad_clip),
            accumulation_steps=training_cfg.get('accumulation_steps', args.accumulation_steps),
            dtype=training_cfg.get('dtype', args.dtype),
            
            # éªŒè¯å’Œä¿å­˜
            eval_interval=training_cfg.get('eval_interval', args.eval_interval),
            save_interval=training_cfg.get('save_interval', args.save_interval),
            log_interval=training_cfg.get('log_interval', args.log_interval),
            early_stopping=training_cfg.get('early_stopping', args.early_stopping),
            patience=training_cfg.get('patience', args.patience),
            
            # è¾“å‡º
            out_dir=training_cfg.get('out_dir', args.out_dir),
            
            # Wandb
            use_wandb=config_dict.get('wandb', {}).get('enable', args.use_wandb),
            wandb_project=config_dict.get('wandb', {}).get('project', args.wandb_project),
            wandb_run_name=config_dict.get('wandb', {}).get('run_name', args.wandb_run_name),
            
            # æ¢å¤
            resume=args.resume,
            resume_from=args.resume_from,
            
            # åˆ†å¸ƒå¼
            ddp=args.ddp,
            local_rank=args.local_rank,
            
            # å…¶ä»–
            num_workers=args.num_workers
        )
    else:
        # ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®
        config = TrainingConfig(
            data_path=args.data_path,
            val_data_path=args.val_data_path,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            min_lr=args.min_lr,
            weight_decay=args.weight_decay,
            optimizer_type=args.optimizer_type,
            lr_scheduler=args.lr_scheduler,
            warmup_ratio=args.warmup_ratio,
            use_ema=args.use_ema,
            grad_clip=args.grad_clip,
            accumulation_steps=args.accumulation_steps,
            dtype=args.dtype,
            eval_interval=args.eval_interval,
            save_interval=args.save_interval,
            log_interval=args.log_interval,
            early_stopping=args.early_stopping,
            patience=args.patience,
            out_dir=args.out_dir,
            use_wandb=args.use_wandb,
            wandb_project=args.wandb_project,
            wandb_run_name=args.wandb_run_name,
            resume=args.resume,
            resume_from=args.resume_from,
            ddp=args.ddp,
            local_rank=args.local_rank,
            num_workers=args.num_workers
        )
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    print(f"ğŸ² éšæœºç§å­: {args.seed}")
    
    # æ‰“å°é…ç½®
    print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  Epochs:          {config.epochs}")
    print(f"  Batch Size:      {config.batch_size}")
    print(f"  Learning Rate:   {config.learning_rate}")
    print(f"  Optimizer:       {config.optimizer_type}")
    print(f"  LR Scheduler:    {config.lr_scheduler}")
    print(f"  Dtype:           {config.dtype}")
    print(f"  Use EMA:         {config.use_ema}")
    print(f"  Early Stopping:  {config.early_stopping}")
    print(f"  Output Dir:      {config.out_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.out_dir, exist_ok=True)
    
    # åˆå§‹åŒ–æ¨¡å‹é…ç½®
    print("\nğŸ—ï¸  åˆå§‹åŒ–æ¨¡å‹...")
    lm_config = LMConfig()
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = Transformer(lm_config)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æ‰“å°æ¨¡å‹ç»“æ„
        print_model_structure(model)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("   è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è„šæœ¬ï¼Œéœ€è¦æ‚¨æä¾›å®é™…çš„æ¨¡å‹ä»£ç ")
        return
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“¦ åŠ è½½æ•°æ®é›†...")
    try:
        # è¿™é‡Œéœ€è¦æ‚¨çš„å®é™…æ•°æ®é›†åŠ è½½ä»£ç 
        train_dataset = PretrainDataset(
            [config.data_path],
            max_length=lm_config.max_seq_len,
            memmap=True
        )
        print(f"âœ… è®­ç»ƒé›†åŠ è½½æˆåŠŸ: {len(train_dataset):,} æ ·æœ¬")
        
        val_dataset = None
        if config.val_data_path:
            val_dataset = PretrainDataset(
                [config.val_data_path],
                max_length=lm_config.max_seq_len,
                memmap=True
            )
            print(f"âœ… éªŒè¯é›†åŠ è½½æˆåŠŸ: {len(val_dataset):,} æ ·æœ¬")
            
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®ä¸”æ•°æ®é›†ç±»å·²æ­£ç¡®å®ç°")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\nâš™ï¸  åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = Trainer(config)
    
    # è®¾ç½®æ¨¡å‹
    trainer.setup_model(model)
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    trainer.setup_optimizer()
    
    # è®¾ç½®æ•°æ®åŠ è½½å™¨
    trainer.setup_dataloaders(train_dataset, val_dataset)
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 80)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    print("=" * 80 + "\n")
    
    try:
        trainer.train()
        print("\nâœ… è®­ç»ƒå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...")
        trainer.save_checkpoint("checkpoint_interrupted.pth")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()

